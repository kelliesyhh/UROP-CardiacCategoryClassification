{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "8e1b0dbaa5766dfa11d81d07f54d8258925fe169f7afed4bf418ffa36fa764f8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Shape of clinParams: (86, 16)\nShape of exercise: (86, 5)\nShape of echo: (86, 37)\nShape of cmr: (86, 27)\nShape of bloodBio: (86, 18)\nShape of physFuncParams: (86, 18)\nShape of curMeta: (86, 89)\nShape of histMeta: (86, 87)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "' \\nfor key in ccee_dfDict:\\n    print(f\"Shape of {key+\\'(EE)\\'}: {cc_dfDict[key].shape}\") \\n'"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "# Cell 1\n",
    "\"\"\"\n",
    "Ideas for a one-size-fits-all pipeline:\n",
    "\n",
    "Check if target column is categorical or continuous\n",
    "Apply from dict of classification or regression models based on above\n",
    "Check if each column is categorical or continous\n",
    "perform one-hot encoding for categorical columns\n",
    "Apply appropriate data visualisations to each\n",
    "Apply correlations Matrix\n",
    "Apply scaling and PCA if too many features (>20?)\n",
    "\n",
    "IMPORTANT: Modify pipeline such that imputation of NA values are done using median of train\n",
    "set AFTER the split. Save this median for use in test set\n",
    "\n",
    "Next steps: \n",
    "- Remove white columns (look at original excel sheet)  (Done)\n",
    "- Group features by colors according to excel sheet (Done)\n",
    "- Check for level of correlation (correlation matrix) (Done)\n",
    "- Visualise correlation matrix (Done)\n",
    "- Apply scaling (standard/minmax) (Done)\n",
    "- Use PCA to reduce dimension (Skipped) https://www.datacamp.com/community/tutorials/principal-component-analysis-in-python\n",
    "- Apply classification algos for each color (Done)\n",
    "- Apply above for everything combined (Done)\n",
    "- Implement hyperparameter tuning (Optional)\n",
    "- Additional data visualisation for each parameter (Split into categorical and continous)(Optional)\n",
    "- Apply unsupervised learning to check for clustering\n",
    "- Cleanup\n",
    "- Write clear comments for future reference\n",
    "- Write and present data nicely\n",
    "\"\"\"\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn \n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVR, SVC\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.model_selection import ShuffleSplit, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression,LogisticRegression \n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor,KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor,DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor,GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor,RandomForestClassifier\n",
    "from lightgbm import LGBMRegressor,LGBMClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, RepeatedStratifiedKFold\n",
    "from tpot import TPOTClassifier, TPOTRegressor\n",
    "from sklearn.cluster import FeatureAgglomeration\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_selection import SelectPercentile, VarianceThreshold, f_regression\n",
    "from sklearn.linear_model import LassoLarsCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.preprocessing import Normalizer, RobustScaler\n",
    "from tpot.builtins import StackingEstimator\n",
    "from tpot.export_utils import set_param_recursive\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from copy import copy\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from tpot import TPOTClassifier\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"kohsm1.csv\",encoding='latin1')\n",
    "\n",
    "#Removal of irrelevant white columns\n",
    "data.drop('IDshort', axis=1, inplace=True)\n",
    "data.drop('SERNO', axis=1, inplace=True)\n",
    "\n",
    "#Full Data containing only each category as targets\n",
    "data_full = data.drop('Cardiac_Category EE (0=good; 1=poor)', axis=1)\n",
    "# dataEE_full = data.drop('Cardiac_Category (0=good; 1=poor)', axis=1)\n",
    "\n",
    "#List of columns according to color category\n",
    "#Blue\n",
    "clinParamsCols=[\"IDshort\",\"Age\",\"Weightkg\",\"Heightcm\",\"Pulse\",\"WaistCircumferencecm\",\"SERNO\",\"Gender\",\"SBP\",\"DBP\",\"Hips_Circumference__cm\",\"Hypertension__Yes_1_No_0\",\"Dyslipidemia__Yes_1_No_0\",\"Diabetes_mellitus__Yes_1_No_0\",\"Smoking__Never_0_Current_1_Past\",\"DiabetesmellitusDietcontrol\",\"AlcoholNever0Current1Past\"]\n",
    "\n",
    "#Teal\n",
    "exerciseCols=[\"PhysicalactivityfrequencyIna\",\"IntensityTakeiteasy1Heavy\",\"Duration15min116to30min\",\"VO2Max\"]\n",
    "\n",
    "#Violet\n",
    "echoCols=[\"BSA__m2\",\"IVSD__cm\",\"IVSS__cm\",\"LVIDD_cm\",\"LVIDS_cm\",\"LVPWD_cm\",\"LVPWS_cm\",\"LVOT__cm\",\"AO_cm\",\"LA_cm\",\"LVEF\",\"LVFS\",\"LVmass_echo\",\"LVmass_index_echo\",\"Left_atrial_volume\",\"Left_atrial_volume_index\",\"MV_E_peak__m_s\",\"MV_A_peak__m_s\",\"E_A_ratio\",\"MV_DT__ms\",\"PASP__mmHg\",\"PVS__cm_s\",\"PVD__cm_s\",\"PVA__cm_s\",\"PVADur\",\"MVADur\",\"septalS\",\"Septal_E\",\"Septal_A\",\"Lateral_S\",\"lateralE\",\"lateralA\",\"sinuscm\",\"sinus_tubular_junctioncm\",\"ave_Eprime\",\"E_Eprime_ratio\"]\n",
    "\n",
    "#Grey\n",
    "cmrCols=[\"LV_Mass_on_mri__g\",\"LV_EDV\",\"EDV067\",\"LV_ESV\",\"LV_SV\",\"LVEF_A\",\"Ss\",\"Se\",\"Sa\",\"SRs\",\"SRe\",\"SRa\",\"SRe_SRaratio\",\"LVGLS\",\"LVGCS\",\"LVGRS\",\"RVGLS\",\"LAGlobalGLS\",\"LAGlobalGCS\",\"LAvolumeminml\",\"LAvolumemaxml\",\"LAEFTotal\",\"LVMassg\",\"LVEDVImlm2\",\"LVESVImlm2\",\"LVSVImlm2\"]\n",
    "\n",
    "#Beige\n",
    "bloodBioCols=[\"MCP1pgmL\",\"MMP9ngmL\",\"BNP\",\"HSTNI\",\"GALECTIN3\",\"sUPAR\",\"qTL\",\"extra_urea\",\"extra_creatinine\",\"extra_total_cholesterol\",\"extra_triglycerides\",\"extra_HDL\",\"extra_LDL\",\"extra_tc_HDL_ratio\",\"extra_albumin\",\"extra_hsCRP\",\"HbA1c\"]\n",
    "\n",
    "#Green\n",
    "physFuncParamsCols=[\"Gripmax\",\"G1L\",\"G2L\",\"G1R\",\"G2R\",\"SMM\",\"BFM\",\"PBF\",\"WHR\",\"Fitness_score\",\"BMR\",\"Lean_LA\",\"Lean_RA\",\"Lean_LL\",\"Lean_RL\",\"Lean_T\",\"ALM\"]\n",
    "\n",
    "#Orange\n",
    "curMetaCols=[\"TC\",\"FC\",\"C2\",\"C3\",\"C4\",\"C51\",\"C5\",\"C4OH\",\"C6\",\"C5OHC3DC\",\"C4DCC6OH\",\"C81\",\"C8\",\"C5DC\",\"C81OHC61DC\",\"C8OHC6DC\",\"C103\",\"C102\",\"C101\",\"C10\",\"C7DC\",\"C81DC\",\"C8DC\",\"C122\",\"C121\",\"C12\",\"C122OHC102DC\",\"C121OH\",\"C12OHC10DC\",\"C143\",\"C142\",\"C141\",\"C14\",\"C143OHC123DC\",\"C142OH\",\"C141OH\",\"C14OHC12DC\",\"C163\",\"C162\",\"C161\",\"C16\",\"C163OHC143DC\",\"C162OH\",\"C161OHC141DC\",\"C16OH\",\"C183\",\"C182\",\"C181\",\"C18\",\"C183OHC163DC\",\"C182OHC162DC\",\"C181OHC161DC\",\"C18OHC16DC\",\"C204\",\"C203\",\"C202\",\"C201\",\"C20\",\"C203OHC183DC\",\"C202OHC182DC\",\"C201OHC181DC\",\"C20OHC18DC\",\"C225\",\"C224\",\"C223\",\"C222\",\"C221\",\"C22\",\"C24\",\"C26\",\"C28\",\"Gly1\",\"Ala1\",\"Ser1\",\"Pro1\",\"Val1\",\"Leu1\",\"IleLeu1\",\"Orn1\",\"Met1\",\"His1\",\"Phe1\",\"Arg1\",\"Cit1\",\"Tyr1\",\"Asp1\",\"Glu1\",\"Trp1\"]\n",
    "\n",
    "#Yellow\n",
    "histMetaCols=[\"HistoC2\",\"HistoC3\",\"HistoC4\",\"HistoC51\",\"HistoC5\",\"HistoC4NDOH\",\"HistoC6\",\"HistoC5NDOHC3NDDC\",\"HistoC4NDDCC6NDOH\",\"HistoC81\",\"HistoC8\",\"HistoC5NDDC\",\"HistoC81NDOHC61NDDC\",\"HistoC8NDOHC6NDDC\",\"HistoC103\",\"HistoC102\",\"HistoC101\",\"HistoC10\",\"HistoC7NDDC\",\"HistoC81NDDC\",\"HistoC8NDDC\",\"HistoC122\",\"HistoC121\",\"HistoC12\",\"HistoC122NDOHC102NDDC\",\"HistoC121NDOH\",\"HistoC12NDOHC10NDDC\",\"HistoC143\",\"HistoC142\",\"HistoC141\",\"HistoC14\",\"HistoC143NDOHC123NDDC\",\"HistoC142NDOH\",\"HistoC141NDOH\",\"HistoC14NDOHC12NDDC\",\"HistoC163\",\"HistoC162\",\"HistoC161\",\"HistoC16\",\"HistoC163NDOHC143NDDC\",\"HistoC162NDOH\",\"HistoC161NDOHC141NDDC\",\"HistoC16NDOH\",\"HistoC183\",\"HistoC182\",\"HistoC181\",\"HistoC18\",\"HistoC183NDOHC163NDDC\",\"HistoC182NDOHC162NDDC\",\"HistoC181NDOHC161NDDC\",\"HistoC18NDOHC16NDDC\",\"HistoC204\",\"HistoC203\",\"HistoC202\",\"HistoC201\",\"HistoC20\",\"HistoC203NDOHC183NDDC\",\"HistoC202NDOHC182NDDC\",\"HistoC201NDOHC181NDDC\",\"HistoC20NDOHC18NDDC\",\"HistoC225\",\"HistoC224\",\"HistoC223\",\"HistoC222\",\"HistoC221\",\"HistoC22\",\"HistoC24\",\"HistoC26\",\"HistoC28\",\"HistoCode\",\"HistoGly1_ConcµM\",\"HistoAla1_ConcµM\",\"HistoSer1_ConcµM\",\"HistoPro1_ConcµM\",\"HistoVal1_ConcµM\",\"HistoLeu1_ConcµM\",\"HistoIle1_ConcµM\",\"HistoOrn1_ConcµM\",\"HistoMet1_ConcµM\",\"HistoHis1_ConcµM\",\"HistoPhe1_ConcµM\",\"HistoCit1_ConcµM\",\"HistoTyr1_ConcµM\",\"HistoAsp1_ConcµM\",\"HistoGlu1_ConcµM\",\"HistoTrp1_ConcµM\"]\n",
    "\n",
    "targets=[\"Cardiac_Category (0=good; 1=poor)\",\"Cardiac_Category EE (0=good; 1=poor)\"]\n",
    "\n",
    "# List comprising of params and Cardiac Category\n",
    "clinParamsColsCC = clinParamsCols + [targets[0]] \n",
    "exerciseColsCC = exerciseCols + [targets[0]]\n",
    "echoColsCC = echoCols + [targets[0]]\n",
    "cmrColsCC = cmrCols + [targets[0]]\n",
    "bloodBioColsCC = bloodBioCols + [targets[0]]\n",
    "physFuncParamsColsCC = physFuncParamsCols + [targets[0]]\n",
    "curMetaColsCC = curMetaCols + [targets[0]]\n",
    "histMetaColsCC = histMetaCols + [targets[0]]\n",
    "\n",
    "# List comprising of params and Cardiac Category_EE\n",
    "\"\"\"\n",
    "clinParamsColsCCEE = clinParamsCols + [targets[1]] \n",
    "histMetaColsCCEE = histMetaCols + [targets[1]]\n",
    "exerciseColsCCEE = exerciseCols + [targets[1]]\n",
    "echoColsCCEE = echoCols + [targets[1]]\n",
    "cmrColsCCEE = cmrCols + [targets[1]]\n",
    "bloodBioColsCCEE = bloodBioCols + [targets[1]]\n",
    "physFuncParamsColsCCEE = physFuncParamsCols + [targets[1]]\n",
    "curMetaColsCCEE = curMetaCols + [targets[1]]\n",
    "\"\"\"\n",
    "# https://stackoverflow.com/questions/48198021/filter-pandas-dataframe-with-specific-column-names-in-python\n",
    "\n",
    "#Dataframe containing parameters and Cardiac Category\n",
    "clinParamsColsCC_df=data.loc[:, data.columns.isin(clinParamsColsCC)]\n",
    "exerciseColsCC_df=data.loc[:, data.columns.isin(exerciseColsCC)]\n",
    "echoColsCC_df=data.loc[:, data.columns.isin(echoColsCC)]\n",
    "cmrColsCC_df=data.loc[:, data.columns.isin(cmrColsCC)]\n",
    "bloodBioColsCC_df=data.loc[:, data.columns.isin(bloodBioColsCC)]\n",
    "physFuncParamsColsCC_df=data.loc[:, data.columns.isin(physFuncParamsColsCC)]\n",
    "curMetaColsCC_df=data.loc[:, data.columns.isin(curMetaColsCC)]\n",
    "histMetaColsCC_df=data.loc[:, data.columns.isin(histMetaColsCC)]\n",
    "\n",
    "#Dataframe containing parameters and Cardiac CategoryEE\n",
    "\"\"\" \n",
    "clinParamsColsCCEE_df=data.loc[:, data.columns.isin(clinParamsColsCCEE)] \n",
    "exerciseColsCCEE_df=data.loc[:, data.columns.isin(exerciseColsCCEE)]\n",
    "echoColsCCEE_df=data.loc[:, data.columns.isin(echoColsCCEE)]\n",
    "cmrColsCCEE_df=data.loc[:, data.columns.isin(cmrColsCCEE)]\n",
    "bloodBioColsCCEE_df=data.loc[:, data.columns.isin(bloodBioColsCCEE)]\n",
    "physFuncParamsColsCCEE_df=data.loc[:, data.columns.isin(physFuncParamsColsCCEE)]\n",
    "curMetaColsCCEE_df=data.loc[:, data.columns.isin(curMetaColsCCEE)]\n",
    "histMetaColsCCEE_df=data.loc[:, data.columns.isin(histMetaColsCCEE)]\n",
    " \"\"\"\n",
    "#Dict of dataframes of each color cateogry for cardiac category\n",
    "cc_dfDict = {\n",
    "    \"clinParams\":clinParamsColsCC_df,\n",
    "    \"exercise\":exerciseColsCC_df,\n",
    "    \"echo\":echoColsCC_df,\n",
    "    \"cmr\":cmrColsCC_df,\n",
    "    \"bloodBio\":bloodBioColsCC_df,\n",
    "    \"physFuncParams\":physFuncParamsColsCC_df,\n",
    "    \"curMeta\":curMetaColsCC_df,\n",
    "    \"histMeta\":histMetaColsCC_df\n",
    "}\n",
    "\n",
    "#Dict of dataframes of each color cateogry for cardiac category EE\n",
    "\"\"\" \n",
    "ccee_dfDict = {\n",
    "    \"clinParams\":clinParamsColsCCEE_df,\n",
    "    \"exercise\":exerciseColsCCEE_df,\n",
    "    \"echo\":echoColsCCEE_df,\n",
    "    \"cmr\":cmrColsCCEE_df,\n",
    "    \"bloodBio\":bloodBioColsCCEE_df,\n",
    "    \"physFuncParams\":physFuncParamsColsCCEE_df,\n",
    "    \"curMeta\":curMetaColsCCEE_df,\n",
    "    \"histMeta\":histMetaColsCCEE_df\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "for key in cc_dfDict:\n",
    "    print(f\"Shape of {key}: {cc_dfDict[key].shape}\")\n",
    "\n",
    "\"\"\" \n",
    "for key in ccee_dfDict:\n",
    "    print(f\"Shape of {key+'(EE)'}: {cc_dfDict[key].shape}\") \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2\n",
    "#https://stackoverflow.com/questions/45515031/how-to-remove-columns-with-too-many-missing-values-in-python\n",
    "\"\"\"\n",
    "def missing(dff):\n",
    "    print (round((dff.isnull().sum() * 100/ len(dff)),2).sort_values(ascending=False))\n",
    "\"\"\"\n",
    "\n",
    "# Helper function\n",
    "def rmissingvaluecol(df, threshold):\n",
    "    # If threshold is 80 it means we are going to drop columns having more than 80% of missing values\n",
    "    l = []\n",
    "    l = list(df.drop(df.loc[:,list((100*(df.isnull().sum()/len(df.index)) >= threshold))].columns, 1).columns.values)\n",
    "    print(\"Number of columns having more than %s percent missing values: \"%threshold, (df.shape[1] - len(l)))\n",
    "    print(\"These columns are:\\n\", list(set(list((df.columns.values))) - set(l)))\n",
    "    # Returns columns that are missing less than threshold % of data\n",
    "    return l\n",
    "    \n",
    "def preprocessDf(df,threshold=40,verbose=False):\n",
    "    print(\"Preprocessing dataframe...\")\n",
    "    if verbose:\n",
    "        print(\"Initial info:\")\n",
    "        print(df.info())\n",
    "    originalShape = df.shape\n",
    "    newDf = df.copy()\n",
    "    \n",
    "    for col in newDf:\n",
    "        #Checks for categorical columns that only have 2 values, and converts them to binary\n",
    "        uniqueVals = pd.unique(newDf[col])\n",
    "        if len(uniqueVals) == 2:\n",
    "            if not(np.int64(0) in uniqueVals and np.int64(1) in uniqueVals):\n",
    "                print(f\"{col} column contains only 2 values. Converting to binary\")\n",
    "                print(uniqueVals[0])\n",
    "                newDf[col] = np.where(newDf[col] == uniqueVals[0],np.int64(0),newDf[col])\n",
    "                newDf[col] = np.where(newDf[col] == uniqueVals[1],np.int64(1),newDf[col])\n",
    "                #Renames column with labels for binary values\n",
    "                newDf.rename(columns={col:col + f\"_{uniqueVals[0]}_0_{uniqueVals[1]}_1\"}, inplace=True)\n",
    "    \n",
    "    # Converting values that are non-numeric to NA\n",
    "    newDf = newDf.apply(pd.to_numeric,errors='coerce')\n",
    "    print(\"Converting values that are non-numeric AND non-binary to NaN...\")\n",
    "    newCols = rmissingvaluecol(newDf,threshold) #Removes columns with more than threshold of missing data\n",
    "    print(\"Columns with missing values (%):\")\n",
    "    newDf_missing = newDf.isna().sum()\n",
    "    newDf_missing = newDf_missing[newDf_missing > 0].dropna().sort_values(ascending=False)\n",
    "    newDf_missing = newDf_missing.apply(lambda x: np.round((x/originalShape[0])*100,2))\n",
    "    print(newDf_missing)\n",
    "    newDf = newDf[newCols]\n",
    "\n",
    "    print(f\"Original Shape:{df.shape}\")\n",
    "    print(f\"Final Shape:{newDf.shape}\")\n",
    "    if verbose:\n",
    "        print(\"Final info:\")\n",
    "        print(newDf.info())\n",
    "    print(\"______________________________\")\n",
    "    return newDf\n",
    "\n",
    "#https://stackoverflow.com/questions/17778394/list-highest-correlation-pairs-from-a-large-correlation-matrix-in-pandas\n",
    "\n",
    "# Helper function for get_top_abs_correlations. returns list of redundant pairs to be dropped\n",
    "def get_redundant_pairs(df):\n",
    "    '''Get diagonal and lower triangular pairs of correlation matrix'''\n",
    "    pairs_to_drop = set()\n",
    "    cols = df.columns\n",
    "    for i in range(0, df.shape[1]):\n",
    "        for j in range(0, i+1):\n",
    "            pairs_to_drop.add((cols[i], cols[j]))\n",
    "    return pairs_to_drop\n",
    "\n",
    "# Function that returns a table of correlation indexes in descending order\n",
    "def get_top_abs_correlations(df, threshold=0.6):\n",
    "    au_corr = df.corr().abs().unstack()\n",
    "    labels_to_drop = get_redundant_pairs(df)\n",
    "    au_corr = au_corr.drop(labels=labels_to_drop).sort_values(ascending=False)\n",
    "    return au_corr[au_corr>=threshold]\n",
    "\n",
    "\n",
    "# Because of mixed data type (Binary and Non-binary), it would probably be \n",
    "# more correct to use Factor analysis of mixed data (FAMD) instead of simple PCA. \n",
    "# https://github.com/MaxHalford/prince#factor-analysis-of-mixed-data-famd\n",
    "\n",
    "# Function currently not being used\n",
    "def scaleAndPca(df,target,explainabilityThreshold = 0.85):\n",
    "    #print(df)\n",
    "    # Removes target column\n",
    "    newDf = df.drop([target], axis=1)\n",
    "    \n",
    "    featuresLength = newDf.shape[1]\n",
    "    \n",
    "    \"\"\"\n",
    "    nonBinary = newDf.columns[~newDf.isin([0,1]).all()]\n",
    "    binary = newDf.columns[newDf.isin([0,1]).all()]\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(newDf)\n",
    "    \n",
    "    newDf = scaler.transform(newDf)\n",
    "    # https://towardsdatascience.com/one-hot-encoding-standardization-pca-data-preparation-steps-for-segmentation-in-python-24d07671cf0b\n",
    "    # Loop Function to identify number of principal components that explain at least threshold% of the variance\n",
    "    for comp in range(1, featuresLength):\n",
    "        pca = PCA(n_components= comp, random_state=0)\n",
    "        pca.fit(newDf)\n",
    "        comp_check = pca.explained_variance_ratio_\n",
    "        final_comp = comp\n",
    "        if comp_check.sum() > explainabilityThreshold:\n",
    "            break \n",
    "    Final_PCA = PCA(n_components= final_comp,random_state=0)\n",
    "    Final_PCA.fit(newDf)\n",
    "    cluster_newDf = Final_PCA.transform(newDf)\n",
    "    cluster_newDf = pd.DataFrame(cluster_newDf) \n",
    "    combinedDf = pd.concat([cluster_newDf, df[target]], axis=1)\n",
    "    num_comps = comp_check.shape[0]\n",
    "    print(f\"Reducing {featuresLength} features to {final_comp} components, we can explain {comp_check.sum()}% of the variability in the original data.\")\n",
    "    return combinedDf\n",
    "\n",
    "#scaleAndPca(cc_dfDictNew[\"clinParams\"],\"Cardiac_Category (0=good; 1=poor)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3\n",
    "# Display correlation data above given threshold\n",
    "def outputCorrData(df,threshold=0.6,filename=\"Output\",generateFiles=False):\n",
    "    df = preprocessDf(df)\n",
    "    df = df.fillna(df.median())\n",
    "    # Creates correlation matrix\n",
    "    corrDf = df.corr()\n",
    "    # Creates corr matrix, showing correlations above given threshold\n",
    "    filteredDf = corrDf[((corrDf >= threshold) | (corrDf <= -threshold)) & (corrDf !=1.000)]\n",
    "    plt.figure(figsize=(20,17))\n",
    "    sn.heatmap(filteredDf, annot=True, cmap=\"Reds\")\n",
    "    plt.title(f\"{filename} Correlation Heatmap (Threshold: >= abs{threshold})\",fontsize=20)\n",
    "    # Returns a table of correlation indexes in descending order\n",
    "    \n",
    "    cmrCorrTable = get_top_abs_correlations(df, threshold).to_frame()\n",
    "    cmrCorrTable.columns =['Abs Corr Idx']\n",
    "    plt.show\n",
    "\n",
    "    # Generates excel and png files of output in your system for future reference\n",
    "    if generateFiles:\n",
    "        if not os.path.exists('Outputs/Plots'):\n",
    "            os.makedirs('Outputs/Plots')\n",
    "        if not os.path.exists('Outputs/Tables'):\n",
    "            os.makedirs('Outputs/Tables')\n",
    "        plt.savefig(f'Outputs/Plots/corr_{filename}_heatmap.png')\n",
    "        cmrCorrTable.to_excel(f'Outputs/Tables/corr_{filename}_table.xlsx') \n",
    "    return corrDf\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_feature_importance(importance,names,title):\n",
    "    # Adapted from https://www.analyseup.com/learn-python-for-data-science/python-random-forest-feature-importance-plot.html\n",
    "    #Create arrays from feature importance and feature names\n",
    "    feature_importance = np.array(importance)\n",
    "    feature_names = np.array(names)\n",
    "\n",
    "    #Create a DataFrame using a Dictionary\n",
    "    data={'feature_names':feature_names,'feature_importance':feature_importance}\n",
    "    fi_df = pd.DataFrame(data)\n",
    "\n",
    "    #Sort the DataFrame in order decreasing feature importance\n",
    "    fi_df.sort_values(by=['feature_importance'], ascending=False,inplace=True)\n",
    "\n",
    "    #Define size of bar plot\n",
    "    plt.figure(figsize=(10,8))\n",
    "    #Plot Searborn bar chart\n",
    "    sn.barplot(x=fi_df['feature_importance'][:5], y=fi_df['feature_names'][:5])\n",
    "    #Add chart labels\n",
    "    plt.title(title)\n",
    "    plt.xlabel('FEATURE IMPORTANCE')\n",
    "    plt.ylabel('FEATURE NAMES')\n",
    "    print(fi_df[:5])\n",
    "    plt.show()\n",
    "    return fi_df # returns full feature importance dataframe\n",
    "\n",
    "#https://analyticsindiamag.com/how-to-implement-ml-models-with-small-datasets/\n",
    "\n",
    "# https://stats.stackexchange.com/questions/416553/can-k-fold-cross-validation-cause-overfitting#:~:text=2%20Answers&text=K%2Dfold%20cross%20validation%20is,fold%20cross%2Dvalidation%20removes%20overfitting.\n",
    "\n",
    "# Adapted from https://www.kaggle.com/brsdincer/heart-attack-prediction-detailed-explanation\n",
    "\n",
    "# k-fold CV is used to DETECT overfitting, since it trains the model from scratch each time \n",
    "\n",
    "def compareClasModel(df,target,test_size=0.25,title=\"Dataset\",showImportance=False):\n",
    "    print(f\"Comparing models for {title}\")\n",
    "    y = df[target]\n",
    "    df = df.drop(target, axis = 1)\n",
    "    x = preprocessDf(df)\n",
    "    \n",
    "    print(\"Initialising models...\")\n",
    "    xTrain,xTest,yTrain,yTest = train_test_split(x,y,test_size=test_size,random_state=0)\n",
    "    xTrain = xTrain.fillna(xTrain.median())\n",
    "    xTest = xTest.fillna(xTrain.median())\n",
    "    #https://stackoverflow.com/questions/18689823/pandas-dataframe-replace-nan-values-with-average-of-columns\n",
    "\n",
    "    lr = LogisticRegression(solver=\"liblinear\").fit(xTrain,yTrain)\n",
    "    gnb = GaussianNB().fit(xTrain,yTrain)\n",
    "    knnc = KNeighborsClassifier().fit(xTrain,yTrain)\n",
    "    dtc = DecisionTreeClassifier(random_state=0).fit(xTrain,yTrain)\n",
    "    rfc = RandomForestClassifier(random_state=0,verbose=False).fit(xTrain,yTrain)\n",
    "    gbmc = GradientBoostingClassifier(verbose=False).fit(xTrain,yTrain)\n",
    "    xgbc = XGBClassifier(use_label_encoder=False,eval_metric= \"error\").fit(xTrain,yTrain)\n",
    "    lgbmc = LGBMClassifier().fit(xTrain,yTrain)\n",
    "    sv = SVC().fit(xTrain,yTrain)\n",
    "\n",
    "    models = [lr,gnb,knnc,dtc,rfc,gbmc,xgbc,lgbmc,sv]\n",
    "    \n",
    "    # initialise empty dataframe\n",
    "    comparison = pd.DataFrame(columns=[\"Model\",\"AccuracyScore\",\"MeanR2CVScore\",\"MeanCVError\"])\n",
    "    # Because dataset is small, we use k-fold cross validation\n",
    "\n",
    "    for model in models:\n",
    "        name = model.__class__.__name__\n",
    "        predict = model.predict(xTest)\n",
    "        accuracy = accuracy_score(yTest,predict)\n",
    "        r2CV = cross_val_score(model,xTest,yTest,cv=5).mean()\n",
    "        error = np.sqrt(-cross_val_score(model,xTest,yTest,cv=5,scoring=\"neg_mean_squared_error\").mean())\n",
    "        result = pd.DataFrame([[name,accuracy,r2CV,error]],columns=[\"Model\",\"AccuracyScore\",\"MeanR2CVScore\",\"MeanCVError\"])\n",
    "        comparison = comparison.append(result)\n",
    "        if showImportance:\n",
    "            try:\n",
    "                importance = model.feature_importances_\n",
    "                plot_feature_importance(importance,xTest.columns.tolist(),name)\n",
    "            except AttributeError:\n",
    "                continue\n",
    "    \n",
    "    print(comparison)\n",
    "    print(\"\\n\")\n",
    "    print(f\"Highest Accuracy Score: {comparison.loc[comparison['AccuracyScore'] == comparison['AccuracyScore'].max(), 'Model'].iloc[0]}. Score: {round(comparison['AccuracyScore'].max(),2)}\")\n",
    "    print(f\"Highest CV Score: {comparison.loc[comparison['MeanR2CVScore'] == comparison['MeanR2CVScore'].max(), 'Model'].iloc[0]}. Score: {round(comparison['MeanR2CVScore'].max(),2)}\")\n",
    "    print(f\"Lowest CV Error: {comparison.loc[comparison['MeanCVError'] == comparison['MeanCVError'].min(), 'Model'].iloc[0]}. Score: {round(comparison['MeanCVError'].min(),2)}\")\n",
    "    print(\"\\n\")\n",
    "    comparisonMelted = pd.melt(comparison,id_vars=['Model'])\n",
    "\n",
    "    #sn.set_theme(style=\"whitegrid\")\n",
    "    plt.ylabel('Returns')\n",
    "    \n",
    "    ax = sn.barplot(x=\"value\", y=\"Model\", hue=\"variable\", data=comparisonMelted)\n",
    "    plt.legend(title='Metrics',bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    ax.set(title=f\"{title}\",ylabel='Model',xlabel='Percentage Score')\n",
    "    plt.xlim(0, 1)\n",
    "    plt.show()\n",
    "    #plt.xticks(rotation=90)\n",
    "    print(\"***********************************************\")\n",
    "    \n",
    "    return comparison\n",
    "    \n",
    "def compareRegModel(df,target,test_size=0.25,title=\"Dataset\",showImportance=False):\n",
    "    print(f\"Comparing models for {title}\")\n",
    "    y = df[target]\n",
    "    df = df.drop(target, axis = 1)\n",
    "    x = preprocessDf(df)\n",
    "    \n",
    "    print(\"Predicting \" + title)\n",
    "    xTrain,xTest,yTrain,yTest = train_test_split(x,y,test_size=test_size,random_state=0)\n",
    "    xTrain = xTrain.fillna(xTrain.median())\n",
    "    xTest = xTest.fillna(xTrain.median())\n",
    "    #https://stackoverflow.com/questions/18689823/pandas-dataframe-replace-nan-values-with-average-of-columns\n",
    "\n",
    "    lr = LinearRegression().fit(xTrain,yTrain)\n",
    "    pls = PLSRegression().fit(xTrain,yTrain)\n",
    "    ridge = Ridge().fit(xTrain,yTrain)\n",
    "    lasso = Lasso().fit(xTrain,yTrain)\n",
    "    elasticnet = ElasticNet().fit(xTrain,yTrain)\n",
    "    knnr = KNeighborsRegressor().fit(xTrain,yTrain)\n",
    "    dtr = DecisionTreeRegressor(random_state=0).fit(xTrain,yTrain)\n",
    "    baggr = BaggingRegressor(random_state=0,bootstrap_features=True,verbose=False).fit(xTrain,yTrain)\n",
    "    rfr = RandomForestRegressor(random_state=0,verbose=False).fit(xTrain,yTrain)\n",
    "    gbmr = GradientBoostingRegressor(verbose=False).fit(xTrain,yTrain)\n",
    "    xgbr = XGBRegressor().fit(xTrain,yTrain)\n",
    "    lgbmr = LGBMRegressor().fit(xTrain,yTrain)\n",
    "    sv = SVR().fit(xTrain,yTrain)\n",
    "\n",
    "    models = [lr,pls,ridge,lasso,elasticnet,knnr,dtr,baggr,rfr,gbmr,xgbr,lgbmr,sv]\n",
    "    \n",
    "    # initialise empty dataframe\n",
    "    comparison = pd.DataFrame(columns=[\"Model\",\"MeanR2CVScore\",\"MeanR2CVError\"])\n",
    "    # Because dataset is small, we use k-fold cross validation\n",
    "\n",
    "    for model in models:\n",
    "        name = model.__class__.__name__\n",
    "        r2CV = cross_val_score(model,xTest,yTest,cv=5,scoring=\"r2\").mean()\n",
    "        error = np.sqrt(-cross_val_score(model,xTest,yTest,cv=5,scoring=\"neg_mean_squared_error\").mean())\n",
    "        result = pd.DataFrame([[name,r2CV,error]],columns=[\"Model\",\"MeanR2CVScore\",\"MeanR2CVError\"])\n",
    "        comparison = comparison.append(result)\n",
    "        if showImportance:\n",
    "            try:\n",
    "                importance = model.feature_importances_\n",
    "                plot_feature_importance(importance,xTest.columns.tolist(),name)\n",
    "            except AttributeError:\n",
    "                continue\n",
    "    \n",
    "    print(comparison)\n",
    "    print(\"\\n\")\n",
    "    print(f\"Highest R2CV Score: {comparison.loc[comparison['MeanR2CVScore'] == comparison['MeanR2CVScore'].max(), 'Model'].iloc[0]}. Score: {round(comparison['MeanR2CVScore'].max(),2)}\")\n",
    "    print(f\"Lowest CV Error: {comparison.loc[comparison['MeanR2CVError'] == comparison['MeanR2CVError'].min(), 'Model'].iloc[0]}. Score: {round(comparison['MeanR2CVError'].min(),2)}\")\n",
    "    print(\"\\n\")\n",
    "    comparison = comparison.drop([\"MeanR2CVError\"],axis = 1)\n",
    "    comparisonMelted = pd.melt(comparison,id_vars=['Model'])\n",
    "\n",
    "    #sn.set_theme(style=\"whitegrid\")\n",
    "    plt.ylabel('Returns')\n",
    "    \n",
    "    ax = sn.barplot(x=\"value\", y=\"Model\", hue=\"variable\", data=comparisonMelted)\n",
    "    plt.legend(title='Metrics',bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    ax.set(title=f\"{title}\",ylabel='Model',xlabel='Values')\n",
    "    plt.xlim(-1,1)\n",
    "    plt.show()\n",
    "    #plt.xticks(rotation=90)\n",
    "    print(\"***********************************************\")\n",
    "    \n",
    "    return comparison\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from tpot import TPOTClassifier\n",
    "\n",
    "target = \"Cardiac_Category (0=good; 1=poor)\"\n",
    "\n",
    "noEAratio = data_full.drop(\"E_A_ratio\",axis = 1)\n",
    "# split into input and output elements\n",
    "y = noEAratio[target]\n",
    "noEAratio = noEAratio.drop(target, axis = 1)\n",
    "x = preprocessDf(noEAratio)\n",
    "x = x.fillna(x.median())\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "# define search\n",
    "model = TPOTClassifier(generations=10, population_size=50, cv=cv, scoring='accuracy', verbosity=2, random_state=1, n_jobs=-1)\n",
    "# perform the search\n",
    "model.fit(x, y)\n",
    "# export the best model\n",
    "model.export('bestTPOTmodel3(Cardiac_Category).py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tpot import TPOTRegressor\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "target = \"E_A_ratio\"\n",
    "\n",
    "noCC = data_full.drop(\"Cardiac_Category (0=good; 1=poor)\",axis = 1)\n",
    "# split into input and output elements\n",
    "y = noCC[target]\n",
    "noCC = noCC.drop(target, axis = 1)\n",
    "x = preprocessDf(noCC)\n",
    "\n",
    "x = x.fillna(x.median())\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# define search\n",
    "model2 = TPOTRegressor(generations=10, population_size=50, scoring='neg_mean_absolute_error', cv=cv, verbosity=2, random_state=1, n_jobs=-1)\n",
    "# perform the search\n",
    "model2.fit(x, y)\n",
    "# export the best model\n",
    "model2.export('bestTPOTmodel(E_A_ratio).py')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.linear_model import LassoLarsCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from tpot.builtins import StackingEstimator\n",
    "from tpot.export_utils import set_param_recursive\n",
    "\n",
    "target = \"E_A_ratio\"\n",
    "\n",
    "\n",
    "noCC = data_full.drop(\"Cardiac_Category (0=good; 1=poor)\",axis = 1)\n",
    "y = noCC[target]\n",
    "noCC = noCC.drop(target, axis = 1)\n",
    "x = preprocessDf(noCC)\n",
    "\n",
    "xTrain,xTest,yTrain,yTest = train_test_split(x,y,test_size=0.25,random_state=1)\n",
    "xTrain = xTrain.fillna(xTrain.median())\n",
    "xTest = xTest.fillna(xTrain.median())\n",
    "\n",
    "# Average CV score on the training set was: -0.04015196862655974\n",
    "exported_pipeline = make_pipeline(\n",
    "    StackingEstimator(estimator=DecisionTreeRegressor(max_depth=1, min_samples_leaf=15, min_samples_split=18)),\n",
    "    VarianceThreshold(threshold=0.0005),\n",
    "    RobustScaler(),\n",
    "    LassoLarsCV(normalize=True)\n",
    ")\n",
    "# Fix random state for all the steps in exported pipeline\n",
    "set_param_recursive(exported_pipeline.steps, 'random_state', 1)\n",
    "\n",
    "exported_pipeline.fit(xTrain, yTrain)\n",
    "\n",
    "results = exported_pipeline.predict(xTest[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xTest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = exported_pipeline.predict(xTest)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tpot import TPOTRegressor\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "target = \"E_A_ratio\"\n",
    "\n",
    "physFuncData = data_full[physFuncParamsCols]\n",
    "y = data_full[target]\n",
    "x = preprocessDf(physFuncData)\n",
    "# split into input and output elements\n",
    "\n",
    "xTrain,xTest,yTrain,yTest = train_test_split(x,y,test_size=0.25,random_state=1)\n",
    "xTrain = xTrain.fillna(xTrain.median())\n",
    "xTest = xTest.fillna(xTrain.median())\n",
    "\n",
    "\n",
    "# Average CV score on the training set was: -0.1808613990329195\n",
    "optimisedModel = GradientBoostingRegressor(alpha=0.9, learning_rate=0.001, loss=\"huber\", max_depth=3, max_features=0.6000000000000001, min_samples_leaf=5, min_samples_split=15, n_estimators=100, subsample=0.3)\n",
    "# Fix random state in exported estimator\n",
    "if hasattr(optimisedModel, 'random_state'):\n",
    "    setattr(optimisedModel, 'random_state', 1)\n",
    "\n",
    "optimisedModel.fit(xTrain, yTrain)\n",
    "results = optimisedModel.predict(xTest)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from tpot import TPOTClassifier\n",
    "\n",
    "target = \"Cardiac_Category (0=good; 1=poor)\"\n",
    "\n",
    "noEchoAndCmr = data_full.drop(echoCols + cmrCols,axis = 1)\n",
    "# split into input and output elements\n",
    "\n",
    "\n",
    "y = noEchoAndCmr[target]\n",
    "noEchoAndCmr = noEchoAndCmr.drop(target, axis = 1)\n",
    "x = preprocessDf(noEchoAndCmr)\n",
    "x = x.fillna(x.median())\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "# define search\n",
    "model = TPOTClassifier(generations=200, population_size=50, cv=cv, scoring='accuracy', verbosity=2, random_state=1, n_jobs=-1)\n",
    "# perform the search\n",
    "model.fit(x, y)\n",
    "# export the best model\n",
    "model.export('noEchoAndCmr(Cardiac_Category).py')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"Cardiac_Category (0=good; 1=poor)\"\n",
    "\n",
    "noEchoAndCmr = data_full.drop(echoCols + cmrCols,axis = 1)\n",
    "# split into input and output elements\n",
    "\n",
    "\n",
    "y = noEchoAndCmr[target]\n",
    "noEchoAndCmr = noEchoAndCmr.drop(target, axis = 1)\n",
    "x = preprocessDf(noEchoAndCmr)\n",
    "\n",
    "xTrain,xTest,yTrain,yTest = train_test_split(x,y,test_size=0.25,random_state=1)\n",
    "xTrain = xTrain.fillna(xTrain.median())\n",
    "xTest = xTest.fillna(xTrain.median())\n",
    "\n",
    "# Average CV score on the training set was: 0.7209150326797386\n",
    "exported_pipeline = make_pipeline(\n",
    "    PolynomialFeatures(degree=2, include_bias=False, interaction_only=False),\n",
    "    RBFSampler(gamma=0.35000000000000003),\n",
    "    DecisionTreeClassifier(criterion=\"gini\", max_depth=1, min_samples_leaf=14, min_samples_split=15)\n",
    ")\n",
    "# Fix random state for all the steps in exported pipeline\n",
    "set_param_recursive(exported_pipeline.steps, 'random_state', 42)\n",
    "\n",
    "exported_pipeline.fit(xTrain, yTrain)\n",
    "results = exported_pipeline.predict(xTest)\n",
    "\n",
    "r2CV = cross_val_score(exported_pipeline,xTest,yTest,cv=10).mean()\n",
    "print(r2CV)\n",
    "\n",
    "print(\"Predictions: \")\n",
    "print(results)\n",
    "print(f\"Answers: \")\n",
    "print(yTest.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from tpot import TPOTRegressor\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "target = \"E_A_ratio\"\n",
    "\n",
    "noCC = data_full.drop(\"Cardiac_Category (0=good; 1=poor)\",axis = 1)\n",
    "noEchoAndCmr = noCC.drop(cmrCols + echoCols,axis=1)\n",
    "# split into input and output elements\n",
    "\n",
    "y = data_full[target]\n",
    "# noCC = noCC.drop(target, axis = 1)\n",
    "x = preprocessDf(noEchoAndCmr)\n",
    "\n",
    "x = x.fillna(x.median())\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "# define search\n",
    "model2 = TPOTRegressor(generations=200, population_size=50, scoring='neg_mean_absolute_error', cv=cv, verbosity=2, random_state=1, n_jobs=-1)\n",
    "# perform the search\n",
    "model2.fit(x, y)\n",
    "# export the best model\n",
    "model2.export('noCmrAndEcho(E_A_ratio)2.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"E_A_ratio\"\n",
    "\n",
    "noCC = data_full.drop(\"Cardiac_Category (0=good; 1=poor)\",axis = 1)\n",
    "noEchoAndCmr = noCC.drop(cmrCols + echoCols,axis=1)\n",
    "# split into input and output elements\n",
    "\n",
    "y = data_full[target]\n",
    "# noCC = noCC.drop(target, axis = 1)\n",
    "x = preprocessDf(noEchoAndCmr)\n",
    "\n",
    "xTrain,xTest,yTrain,yTest = train_test_split(x,y,test_size=0.25,random_state=42)\n",
    "xTrain = xTrain.fillna(xTrain.median())\n",
    "xTest = xTest.fillna(xTrain.median())\n",
    "\n",
    "# Average CV score on the training set was: -0.155515661171092\n",
    "exported_pipeline = make_pipeline(\n",
    "    make_union(\n",
    "        make_union(\n",
    "            make_pipeline(\n",
    "                make_union(\n",
    "                    FunctionTransformer(copy),\n",
    "                    make_union(\n",
    "                        RobustScaler(),\n",
    "                        FunctionTransformer(copy)\n",
    "                    )\n",
    "                ),\n",
    "                SelectPercentile(score_func=f_regression, percentile=16),\n",
    "                StackingEstimator(estimator=LassoLarsCV(normalize=True)),\n",
    "                VarianceThreshold(threshold=0.0001)\n",
    "            ),\n",
    "            StackingEstimator(estimator=make_pipeline(\n",
    "                make_union(\n",
    "                    FunctionTransformer(copy),\n",
    "                    make_pipeline(\n",
    "                        make_union(\n",
    "                            FeatureAgglomeration(affinity=\"manhattan\", linkage=\"complete\"),\n",
    "                            FunctionTransformer(copy)\n",
    "                        ),\n",
    "                        Normalizer(norm=\"l2\")\n",
    "                    )\n",
    "                ),\n",
    "                LassoLarsCV(normalize=True)\n",
    "            ))\n",
    "        ),\n",
    "        FunctionTransformer(copy)\n",
    "    ),\n",
    "    GradientBoostingRegressor(alpha=0.99, learning_rate=0.01, loss=\"huber\", max_depth=9, max_features=0.9500000000000001, min_samples_leaf=3, min_samples_split=15, n_estimators=100, subsample=0.9500000000000001)\n",
    ")\n",
    "# Fix random state for all the steps in exported pipeline\n",
    "set_param_recursive(exported_pipeline.steps, 'random_state', 1)\n",
    "\n",
    "\n",
    "exported_pipeline.fit(xTrain, yTrain)\n",
    "\n",
    "results = exported_pipeline.predict(xTest)\n",
    "r2CV = cross_val_score(exported_pipeline,xTest,yTest,cv=5,scoring=\"r2\").mean()\n",
    "print(r2CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results)\n",
    "print(yTest.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[target] = y\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"Cardiac_Category (0=good; 1=poor)\"\n",
    "\n",
    "noEchoAndCmr = data_full.drop(echoCols + cmrCols,axis = 1)\n",
    "# split into input and output elements\n",
    "\n",
    "\n",
    "y = noEchoAndCmr[target]\n",
    "noEchoAndCmr = noEchoAndCmr.drop(target, axis = 1)\n",
    "x = preprocessDf(noEchoAndCmr)\n",
    "\n",
    "xTrain,xTest,yTrain,yTest = train_test_split(x,y,test_size=0.25,random_state=42)\n",
    "xTrain = xTrain.fillna(xTrain.median())\n",
    "xTest = xTest.fillna(xTrain.median())\n",
    "\n",
    "# Average CV score on the training set was: 0.7681917211328977\n",
    "exported_pipeline = make_pipeline(\n",
    "    make_union(\n",
    "        FunctionTransformer(copy),\n",
    "        RobustScaler()\n",
    "    ),\n",
    "    FeatureAgglomeration(affinity=\"manhattan\", linkage=\"average\"),\n",
    "    StackingEstimator(estimator=SGDClassifier(alpha=0.001, eta0=0.1, fit_intercept=False, l1_ratio=0.5, learning_rate=\"invscaling\", loss=\"hinge\", penalty=\"elasticnet\", power_t=0.5)),\n",
    "    RBFSampler(gamma=0.15000000000000002),\n",
    "    RandomForestClassifier(bootstrap=False, criterion=\"entropy\", max_features=0.8, min_samples_leaf=7, min_samples_split=20, n_estimators=100)\n",
    ")\n",
    "# Fix random state for all the steps in exported pipeline\n",
    "set_param_recursive(exported_pipeline.steps, 'random_state', 42)\n",
    "\n",
    "exported_pipeline.fit(xTrain, yTrain)\n",
    "results = exported_pipeline.predict(xTest)\n",
    "\n",
    "r2CV = cross_val_score(exported_pipeline,xTest,yTest,cv=5).mean()\n",
    "print(r2CV)\n",
    "\n",
    "print(\"Predictions: \")\n",
    "print(results)\n",
    "print(f\"Answers: \")\n",
    "print(yTest.to_numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Preprocessing dataframe...\n",
      "Converting values that are non-numeric AND non-binary to NaN...\n",
      "Number of columns having more than 40 percent missing values:  0\n",
      "These columns are:\n",
      " []\n",
      "Columns with missing values (%):\n",
      "BNP          5.81\n",
      "GALECTIN3    5.81\n",
      "HSTNI        4.65\n",
      "MMP9ngmL     2.33\n",
      "sUPAR        1.16\n",
      "qTL          1.16\n",
      "dtype: float64\n",
      "Original Shape:(86, 17)\n",
      "Final Shape:(86, 17)\n",
      "______________________________\n",
      "(86, 17)\n",
      "(86,)\n",
      "\n",
      "Generation 1 - Current best internal CV score: 0.6479302832244008\n",
      "\n",
      "Generation 2 - Current best internal CV score: 0.6479302832244008\n",
      "\n",
      "Generation 3 - Current best internal CV score: 0.6479302832244008\n",
      "\n",
      "Generation 4 - Current best internal CV score: 0.6479302832244008\n",
      "\n",
      "Generation 5 - Current best internal CV score: 0.6479302832244008\n",
      "\n",
      "Generation 6 - Current best internal CV score: 0.6479302832244008\n",
      "\n",
      "Generation 7 - Current best internal CV score: 0.6479302832244008\n",
      "\n",
      "Generation 8 - Current best internal CV score: 0.6479302832244008\n",
      "\n",
      "Generation 9 - Current best internal CV score: 0.6511982570806099\n",
      "\n",
      "Generation 10 - Current best internal CV score: 0.6518518518518518\n",
      "\n",
      "Generation 11 - Current best internal CV score: 0.6518518518518518\n",
      "\n",
      "Generation 12 - Current best internal CV score: 0.6518518518518518\n",
      "\n",
      "Generation 13 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 14 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 15 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 16 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 17 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 18 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 19 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 20 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 21 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 22 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 23 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 24 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 25 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 26 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 27 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 28 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 29 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 30 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 31 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 32 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 33 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 34 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 35 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 36 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 37 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 38 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 39 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 40 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 41 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 42 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 43 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 44 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 45 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 46 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 47 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 48 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 49 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 50 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 51 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 52 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 53 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 54 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 55 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 56 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 57 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 58 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 59 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 60 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 61 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 62 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 63 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 64 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 65 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 66 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 67 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 68 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 69 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 70 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 71 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 72 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 73 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 74 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 75 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 76 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 77 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 78 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 79 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 80 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 81 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 82 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 83 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 84 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 85 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 86 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 87 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 88 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 89 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 90 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 91 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 92 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 93 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 94 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 95 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 96 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 97 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 98 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 99 - Current best internal CV score: 0.69760348583878\n",
      "\n",
      "Generation 100 - Current best internal CV score: 0.7010893246187363\n",
      "\n",
      "Generation 101 - Current best internal CV score: 0.7010893246187363\n",
      "\n",
      "Generation 102 - Current best internal CV score: 0.7010893246187363\n",
      "\n",
      "Generation 103 - Current best internal CV score: 0.7010893246187363\n",
      "\n",
      "Generation 104 - Current best internal CV score: 0.7010893246187363\n",
      "\n",
      "Generation 105 - Current best internal CV score: 0.7010893246187363\n",
      "\n",
      "Generation 106 - Current best internal CV score: 0.7010893246187363\n",
      "\n",
      "Generation 107 - Current best internal CV score: 0.7050108932461873\n",
      "\n",
      "Generation 108 - Current best internal CV score: 0.7050108932461873\n",
      "\n",
      "Generation 109 - Current best internal CV score: 0.7050108932461873\n",
      "\n",
      "Generation 110 - Current best internal CV score: 0.7050108932461873\n",
      "\n",
      "Generation 111 - Current best internal CV score: 0.7050108932461873\n",
      "\n",
      "Generation 112 - Current best internal CV score: 0.7050108932461873\n",
      "\n",
      "Generation 113 - Current best internal CV score: 0.7050108932461873\n",
      "\n",
      "Generation 114 - Current best internal CV score: 0.7050108932461873\n",
      "\n",
      "Generation 115 - Current best internal CV score: 0.7058823529411763\n",
      "\n",
      "Generation 116 - Current best internal CV score: 0.709368191721133\n",
      "\n",
      "Generation 117 - Current best internal CV score: 0.709368191721133\n",
      "\n",
      "Generation 118 - Current best internal CV score: 0.709368191721133\n",
      "\n",
      "Generation 119 - Current best internal CV score: 0.709368191721133\n",
      "\n",
      "Generation 120 - Current best internal CV score: 0.709368191721133\n",
      "\n",
      "Generation 121 - Current best internal CV score: 0.709368191721133\n",
      "\n",
      "Generation 122 - Current best internal CV score: 0.7174291938997821\n",
      "\n",
      "Generation 123 - Current best internal CV score: 0.7174291938997821\n",
      "\n",
      "Generation 124 - Current best internal CV score: 0.7174291938997821\n",
      "\n",
      "Generation 125 - Current best internal CV score: 0.7174291938997821\n",
      "\n",
      "Generation 126 - Current best internal CV score: 0.7174291938997821\n",
      "\n",
      "Generation 127 - Current best internal CV score: 0.7174291938997821\n",
      "\n",
      "Generation 128 - Current best internal CV score: 0.7174291938997821\n",
      "\n",
      "Generation 129 - Current best internal CV score: 0.7174291938997821\n",
      "\n",
      "Generation 130 - Current best internal CV score: 0.7213507625272331\n",
      "\n",
      "Generation 131 - Current best internal CV score: 0.7213507625272331\n",
      "\n",
      "Generation 132 - Current best internal CV score: 0.7213507625272331\n",
      "\n",
      "Generation 133 - Current best internal CV score: 0.725272331154684\n",
      "\n",
      "Generation 134 - Current best internal CV score: 0.725272331154684\n",
      "\n",
      "Generation 135 - Current best internal CV score: 0.725272331154684\n",
      "\n",
      "Generation 136 - Current best internal CV score: 0.725272331154684\n",
      "\n",
      "Generation 137 - Current best internal CV score: 0.725272331154684\n",
      "\n",
      "Generation 138 - Current best internal CV score: 0.725272331154684\n",
      "\n",
      "Generation 139 - Current best internal CV score: 0.725272331154684\n",
      "\n",
      "Generation 140 - Current best internal CV score: 0.725272331154684\n",
      "\n",
      "Generation 141 - Current best internal CV score: 0.725272331154684\n",
      "\n",
      "Generation 142 - Current best internal CV score: 0.725272331154684\n",
      "\n",
      "Generation 143 - Current best internal CV score: 0.725272331154684\n",
      "\n",
      "Generation 144 - Current best internal CV score: 0.725272331154684\n",
      "\n",
      "Generation 145 - Current best internal CV score: 0.737037037037037\n",
      "\n",
      "Generation 146 - Current best internal CV score: 0.737037037037037\n",
      "\n",
      "Generation 147 - Current best internal CV score: 0.737037037037037\n",
      "\n",
      "Generation 148 - Current best internal CV score: 0.737037037037037\n",
      "\n",
      "Generation 149 - Current best internal CV score: 0.737037037037037\n",
      "\n",
      "Generation 150 - Current best internal CV score: 0.737037037037037\n",
      "\n",
      "Generation 151 - Current best internal CV score: 0.737037037037037\n",
      "\n",
      "Generation 152 - Current best internal CV score: 0.737037037037037\n",
      "\n",
      "Generation 153 - Current best internal CV score: 0.737037037037037\n",
      "\n",
      "Generation 154 - Current best internal CV score: 0.737037037037037\n",
      "\n",
      "Generation 155 - Current best internal CV score: 0.737037037037037\n",
      "\n",
      "Generation 156 - Current best internal CV score: 0.737037037037037\n",
      "\n",
      "Generation 157 - Current best internal CV score: 0.737037037037037\n",
      "\n",
      "Generation 158 - Current best internal CV score: 0.737037037037037\n",
      "\n",
      "Generation 159 - Current best internal CV score: 0.737037037037037\n",
      "\n",
      "Generation 160 - Current best internal CV score: 0.737037037037037\n",
      "\n",
      "Generation 161 - Current best internal CV score: 0.737037037037037\n",
      "\n",
      "Generation 162 - Current best internal CV score: 0.737037037037037\n",
      "\n",
      "Generation 163 - Current best internal CV score: 0.737037037037037\n",
      "\n",
      "Generation 164 - Current best internal CV score: 0.737037037037037\n",
      "\n",
      "Generation 165 - Current best internal CV score: 0.737037037037037\n",
      "\n",
      "Generation 166 - Current best internal CV score: 0.737037037037037\n",
      "\n",
      "Generation 167 - Current best internal CV score: 0.737037037037037\n",
      "\n",
      "Generation 168 - Current best internal CV score: 0.737037037037037\n",
      "\n",
      "Generation 169 - Current best internal CV score: 0.737037037037037\n",
      "\n",
      "Generation 170 - Current best internal CV score: 0.740958605664488\n",
      "\n",
      "Generation 171 - Current best internal CV score: 0.740958605664488\n",
      "\n",
      "Generation 172 - Current best internal CV score: 0.740958605664488\n",
      "\n",
      "Generation 173 - Current best internal CV score: 0.740958605664488\n",
      "\n",
      "Generation 174 - Current best internal CV score: 0.740958605664488\n",
      "\n",
      "Generation 175 - Current best internal CV score: 0.740958605664488\n",
      "\n",
      "Generation 176 - Current best internal CV score: 0.740958605664488\n",
      "\n",
      "Generation 177 - Current best internal CV score: 0.740958605664488\n",
      "\n",
      "Generation 178 - Current best internal CV score: 0.740958605664488\n",
      "\n",
      "Generation 179 - Current best internal CV score: 0.740958605664488\n",
      "\n",
      "Generation 180 - Current best internal CV score: 0.740958605664488\n",
      "\n",
      "Generation 181 - Current best internal CV score: 0.740958605664488\n",
      "\n",
      "Generation 182 - Current best internal CV score: 0.740958605664488\n",
      "\n",
      "Generation 183 - Current best internal CV score: 0.740958605664488\n",
      "\n",
      "Generation 184 - Current best internal CV score: 0.740958605664488\n",
      "\n",
      "Generation 185 - Current best internal CV score: 0.740958605664488\n",
      "\n",
      "Generation 186 - Current best internal CV score: 0.740958605664488\n",
      "\n",
      "Generation 187 - Current best internal CV score: 0.740958605664488\n",
      "\n",
      "Generation 188 - Current best internal CV score: 0.740958605664488\n",
      "\n",
      "Generation 189 - Current best internal CV score: 0.740958605664488\n",
      "\n",
      "Generation 190 - Current best internal CV score: 0.740958605664488\n",
      "\n",
      "Generation 191 - Current best internal CV score: 0.740958605664488\n",
      "\n",
      "Generation 192 - Current best internal CV score: 0.740958605664488\n",
      "\n",
      "Generation 193 - Current best internal CV score: 0.740958605664488\n",
      "\n",
      "Generation 194 - Current best internal CV score: 0.740958605664488\n",
      "\n",
      "Generation 195 - Current best internal CV score: 0.740958605664488\n",
      "\n",
      "Generation 196 - Current best internal CV score: 0.740958605664488\n",
      "\n",
      "Generation 197 - Current best internal CV score: 0.740958605664488\n",
      "\n",
      "Generation 198 - Current best internal CV score: 0.740958605664488\n",
      "\n",
      "Generation 199 - Current best internal CV score: 0.740958605664488\n",
      "\n",
      "Generation 200 - Current best internal CV score: 0.740958605664488\n",
      "\n",
      "Best pipeline: GradientBoostingClassifier(XGBClassifier(Normalizer(RandomForestClassifier(input_matrix, bootstrap=True, criterion=entropy, max_features=0.05, min_samples_leaf=20, min_samples_split=13, n_estimators=100), norm=max), learning_rate=0.5, max_depth=3, min_child_weight=4, n_estimators=100, n_jobs=1, subsample=0.8500000000000001, verbosity=0), learning_rate=0.1, max_depth=3, max_features=0.6000000000000001, min_samples_leaf=13, min_samples_split=19, n_estimators=100, subsample=0.6500000000000001)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from tpot import TPOTClassifier\n",
    "\n",
    "target = \"Cardiac_Category (0=good; 1=poor)\"\n",
    "\n",
    "bloodBioData = data_full[bloodBioCols + [target]]\n",
    "# split into input and output elements\n",
    "\n",
    "\n",
    "y = bloodBioData[target]\n",
    "bloodBioData = bloodBioData.drop(target, axis = 1)\n",
    "x = preprocessDf(bloodBioData)\n",
    "x = x.fillna(x.median())\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "# define search\n",
    "model = TPOTClassifier(generations=200, population_size=50, max_time_mins=720,cv=cv, scoring='accuracy', verbosity=2, random_state=1, n_jobs=1)\n",
    "# perform the search\n",
    "model.fit(x, y)\n",
    "# export the best model\n",
    "model.export('bloodBioData(Cardiac_Category).py')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Preprocessing dataframe...\n",
      "Converting values that are non-numeric AND non-binary to NaN...\n",
      "Number of columns having more than 40 percent missing values:  0\n",
      "These columns are:\n",
      " []\n",
      "Columns with missing values (%):\n",
      "BNP          5.81\n",
      "GALECTIN3    5.81\n",
      "HSTNI        4.65\n",
      "MMP9ngmL     2.33\n",
      "sUPAR        1.16\n",
      "qTL          1.16\n",
      "dtype: float64\n",
      "Original Shape:(86, 17)\n",
      "Final Shape:(86, 17)\n",
      "______________________________\n",
      "C:\\Users\\Bryan\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\Bryan\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\Bryan\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\Bryan\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\Bryan\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\Bryan\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "[0.5        0.57142857 0.57142857]\n",
      "0.5476190476190476\n",
      "Accuracy: \n",
      "0.7727272727272727\n",
      "Predictions: \n",
      "[1 1 1 1 0 0 1 1 1 0 1 0 1 0 0 1 0 0 0 1 1 1]\n",
      "Answers: \n",
      "[0 1 1 0 0 0 1 1 1 0 1 0 0 0 1 1 0 0 1 1 1 1]\n",
      "C:\\Users\\Bryan\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "target = \"Cardiac_Category (0=good; 1=poor)\"\n",
    "\n",
    "bloodBioData = data_full[bloodBioCols + [target]]\n",
    "# split into input and output elements\n",
    "\n",
    "\n",
    "y = bloodBioData[target]\n",
    "bloodBioData = bloodBioData.drop(target, axis = 1)\n",
    "x = preprocessDf(bloodBioData)\n",
    "xTrain,xTest,yTrain,yTest = train_test_split(x,y,test_size=0.25,random_state=42)\n",
    "\n",
    "xTrain = xTrain.fillna(xTrain.median())\n",
    "xTest = xTest.fillna(xTrain.median())\n",
    "\n",
    "# Average CV score on the training set was: 0.740958605664488\n",
    "exported_pipeline = make_pipeline(\n",
    "    StackingEstimator(estimator=RandomForestClassifier(bootstrap=True, criterion=\"entropy\", max_features=0.05, min_samples_leaf=20, min_samples_split=13, n_estimators=100)),\n",
    "    Normalizer(norm=\"max\"),\n",
    "    StackingEstimator(estimator=XGBClassifier(learning_rate=0.5, max_depth=3, min_child_weight=4, n_estimators=100, n_jobs=1, subsample=0.8500000000000001, verbosity=0)),\n",
    "    GradientBoostingClassifier(learning_rate=0.1, max_depth=3, max_features=0.6000000000000001, min_samples_leaf=13, min_samples_split=19, n_estimators=100, subsample=0.6500000000000001)\n",
    ")\n",
    "# Fix random state for all the steps in exported pipeline\n",
    "set_param_recursive(exported_pipeline.steps, 'random_state', 42)\n",
    "\n",
    "exported_pipeline.fit(xTrain, yTrain)\n",
    "results = exported_pipeline.predict(xTest)\n",
    "\n",
    "r2CV = cross_val_score(exported_pipeline,xTest,yTest,cv=3).mean()\n",
    "print(cross_val_score(exported_pipeline,xTest,yTest,cv=3))\n",
    "print(r2CV)\n",
    "\n",
    "accuracy = accuracy_score(yTest,results)\n",
    "\n",
    "print(\"Accuracy: \")\n",
    "print(accuracy)\n",
    "\n",
    "print(\"Predictions: \")\n",
    "print(results)\n",
    "print(f\"Answers: \")\n",
    "print(yTest.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Preprocessing dataframe...\n",
      "Converting values that are non-numeric AND non-binary to NaN...\n",
      "Number of columns having more than 40 percent missing values:  0\n",
      "These columns are:\n",
      " []\n",
      "Columns with missing values (%):\n",
      "C28    13.95\n",
      "dtype: float64\n",
      "Original Shape:(86, 16)\n",
      "Final Shape:(86, 16)\n",
      "______________________________\n",
      "(86, 16)\n",
      "(86,)\n",
      "\n",
      "Generation 1 - Current best internal CV score: 0.7252723311546843\n",
      "\n",
      "Generation 2 - Current best internal CV score: 0.7446623093681917\n",
      "\n",
      "Generation 3 - Current best internal CV score: 0.7446623093681917\n",
      "\n",
      "Generation 4 - Current best internal CV score: 0.7446623093681917\n",
      "\n",
      "Generation 5 - Current best internal CV score: 0.7446623093681917\n",
      "\n",
      "Generation 6 - Current best internal CV score: 0.7446623093681917\n",
      "\n",
      "Generation 7 - Current best internal CV score: 0.7446623093681917\n",
      "\n",
      "Generation 8 - Current best internal CV score: 0.7446623093681917\n",
      "\n",
      "Generation 9 - Current best internal CV score: 0.7527233115468411\n",
      "\n",
      "Generation 10 - Current best internal CV score: 0.7527233115468411\n",
      "\n",
      "Generation 11 - Current best internal CV score: 0.7529411764705884\n",
      "\n",
      "Generation 12 - Current best internal CV score: 0.7529411764705884\n",
      "\n",
      "Generation 13 - Current best internal CV score: 0.7566448801742922\n",
      "\n",
      "Generation 14 - Current best internal CV score: 0.7566448801742922\n",
      "\n",
      "Generation 15 - Current best internal CV score: 0.7566448801742922\n",
      "\n",
      "Generation 16 - Current best internal CV score: 0.7566448801742922\n",
      "\n",
      "Generation 17 - Current best internal CV score: 0.7607843137254904\n",
      "\n",
      "Generation 18 - Current best internal CV score: 0.7610021786492376\n",
      "\n",
      "Generation 19 - Current best internal CV score: 0.7610021786492376\n",
      "\n",
      "Generation 20 - Current best internal CV score: 0.7610021786492376\n",
      "\n",
      "Generation 21 - Current best internal CV score: 0.7647058823529413\n",
      "\n",
      "Generation 22 - Current best internal CV score: 0.7647058823529413\n",
      "\n",
      "Generation 23 - Current best internal CV score: 0.7649237472766884\n",
      "\n",
      "Generation 24 - Current best internal CV score: 0.7649237472766884\n",
      "\n",
      "Generation 25 - Current best internal CV score: 0.7725490196078433\n",
      "\n",
      "Generation 26 - Current best internal CV score: 0.784095860566449\n",
      "\n",
      "Generation 27 - Current best internal CV score: 0.784095860566449\n",
      "\n",
      "Generation 28 - Current best internal CV score: 0.784095860566449\n",
      "\n",
      "Generation 29 - Current best internal CV score: 0.784095860566449\n",
      "\n",
      "Generation 30 - Current best internal CV score: 0.784095860566449\n",
      "\n",
      "Generation 31 - Current best internal CV score: 0.784095860566449\n",
      "\n",
      "Generation 32 - Current best internal CV score: 0.784095860566449\n",
      "\n",
      "Generation 33 - Current best internal CV score: 0.784095860566449\n",
      "\n",
      "Generation 34 - Current best internal CV score: 0.784095860566449\n",
      "\n",
      "Generation 35 - Current best internal CV score: 0.784095860566449\n",
      "\n",
      "Generation 36 - Current best internal CV score: 0.784095860566449\n",
      "\n",
      "Generation 37 - Current best internal CV score: 0.7915032679738564\n",
      "\n",
      "Generation 38 - Current best internal CV score: 0.7915032679738564\n",
      "\n",
      "Generation 39 - Current best internal CV score: 0.8069716775599128\n",
      "\n",
      "Generation 40 - Current best internal CV score: 0.8069716775599128\n",
      "\n",
      "Generation 41 - Current best internal CV score: 0.8069716775599128\n",
      "\n",
      "Generation 42 - Current best internal CV score: 0.8069716775599128\n",
      "\n",
      "Generation 43 - Current best internal CV score: 0.8069716775599128\n",
      "\n",
      "Generation 44 - Current best internal CV score: 0.8069716775599128\n",
      "\n",
      "Generation 45 - Current best internal CV score: 0.8069716775599128\n",
      "\n",
      "Generation 46 - Current best internal CV score: 0.8069716775599128\n",
      "\n",
      "Generation 47 - Current best internal CV score: 0.8069716775599128\n",
      "\n",
      "Generation 48 - Current best internal CV score: 0.8069716775599128\n",
      "\n",
      "Generation 49 - Current best internal CV score: 0.8069716775599128\n",
      "\n",
      "Generation 50 - Current best internal CV score: 0.8069716775599128\n",
      "\n",
      "Generation 51 - Current best internal CV score: 0.8069716775599128\n",
      "\n",
      "Generation 52 - Current best internal CV score: 0.8069716775599128\n",
      "\n",
      "Generation 53 - Current best internal CV score: 0.8069716775599128\n",
      "\n",
      "Generation 54 - Current best internal CV score: 0.8069716775599128\n",
      "\n",
      "Generation 55 - Current best internal CV score: 0.8069716775599128\n",
      "\n",
      "Generation 56 - Current best internal CV score: 0.8069716775599128\n",
      "\n",
      "Generation 57 - Current best internal CV score: 0.8069716775599128\n",
      "\n",
      "Generation 58 - Current best internal CV score: 0.8069716775599128\n",
      "\n",
      "Generation 59 - Current best internal CV score: 0.8069716775599128\n",
      "\n",
      "Generation 60 - Current best internal CV score: 0.8069716775599128\n",
      "\n",
      "Generation 61 - Current best internal CV score: 0.8069716775599128\n",
      "\n",
      "Generation 62 - Current best internal CV score: 0.8069716775599128\n",
      "\n",
      "Generation 63 - Current best internal CV score: 0.8069716775599128\n",
      "\n",
      "Generation 64 - Current best internal CV score: 0.8069716775599128\n",
      "\n",
      "Generation 65 - Current best internal CV score: 0.8069716775599128\n",
      "\n",
      "Generation 66 - Current best internal CV score: 0.8069716775599128\n",
      "\n",
      "Generation 67 - Current best internal CV score: 0.8069716775599128\n",
      "\n",
      "Generation 68 - Current best internal CV score: 0.8069716775599128\n",
      "\n",
      "Generation 69 - Current best internal CV score: 0.8069716775599128\n",
      "\n",
      "Generation 70 - Current best internal CV score: 0.8069716775599128\n",
      "\n",
      "Generation 71 - Current best internal CV score: 0.8069716775599128\n",
      "\n",
      "Generation 72 - Current best internal CV score: 0.8069716775599128\n",
      "\n",
      "Generation 73 - Current best internal CV score: 0.8069716775599128\n",
      "\n",
      "Generation 74 - Current best internal CV score: 0.8069716775599128\n",
      "\n",
      "Generation 75 - Current best internal CV score: 0.8069716775599128\n",
      "\n",
      "Generation 76 - Current best internal CV score: 0.8069716775599128\n",
      "\n",
      "Generation 77 - Current best internal CV score: 0.8069716775599128\n",
      "\n",
      "Generation 78 - Current best internal CV score: 0.8069716775599128\n",
      "\n",
      "Generation 79 - Current best internal CV score: 0.8069716775599128\n",
      "\n",
      "Generation 80 - Current best internal CV score: 0.8069716775599128\n",
      "\n",
      "\n",
      "TPOT closed during evaluation in one generation.\n",
      "WARNING: TPOT may not provide a good pipeline if TPOT is stopped/interrupted in a early generation.\n",
      "\n",
      "\n",
      "TPOT closed prematurely. Will use the current best pipeline.\n",
      "\n",
      "Best pipeline: GradientBoostingClassifier(ExtraTreesClassifier(input_matrix, bootstrap=False, criterion=entropy, max_features=0.35000000000000003, min_samples_leaf=1, min_samples_split=4, n_estimators=100), learning_rate=0.01, max_depth=8, max_features=0.55, min_samples_leaf=1, min_samples_split=6, n_estimators=100, subsample=0.6000000000000001)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from tpot import TPOTClassifier\n",
    "\n",
    "target = \"Cardiac_Category (0=good; 1=poor)\"\n",
    "\n",
    "selfMadeCols = [\"WaistCircumferencecm\",\"Hips_Circumference__cm\",\"C28\",\"Trp1\",\"HistoC101\",\"HistoC143NDOHC123NDDC\",\"HistoC162\",\"HistoC163\",\"HistoC142\",\"HistoC141\",\"HistoC143\",\"HistoHis1_ConcµM\",\"HistoC225\",\"HistoC203\",\"HistoC182\",\"HistoC181\",\"Cardiac_Category (0=good; 1=poor)\"]\n",
    "curatedDf = data_full[selfMadeCols]\n",
    "\n",
    "\n",
    "\n",
    "y = curatedDf[target]\n",
    "curatedDf = curatedDf.drop(target, axis = 1)\n",
    "x = preprocessDf(curatedDf)\n",
    "x = x.fillna(x.median())\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "# define search\n",
    "model = TPOTClassifier(generations=200, population_size=50, max_time_mins=720,cv=cv, scoring='accuracy', verbosity=2, random_state=1, n_jobs=1)\n",
    "# perform the search\n",
    "model.fit(x, y)\n",
    "# export the best model\n",
    "model.export('curatedDf(Cardiac_Category).py')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Preprocessing dataframe...\n",
      "Converting values that are non-numeric AND non-binary to NaN...\n",
      "Number of columns having more than 40 percent missing values:  0\n",
      "These columns are:\n",
      " []\n",
      "Columns with missing values (%):\n",
      "C28     13.95\n",
      "WHR      5.81\n",
      "C222     1.16\n",
      "C101     1.16\n",
      "dtype: float64\n",
      "Original Shape:(86, 17)\n",
      "Final Shape:(86, 17)\n",
      "______________________________\n",
      "(86, 17)\n",
      "(86,)\n",
      "\n",
      "Generation 1 - Current best internal CV score: 0.7673856209150327\n",
      "\n",
      "Generation 2 - Current best internal CV score: 0.7673856209150327\n",
      "\n",
      "Generation 3 - Current best internal CV score: 0.7703267973856209\n",
      "\n",
      "Generation 4 - Current best internal CV score: 0.7703267973856209\n",
      "\n",
      "Generation 5 - Current best internal CV score: 0.7703267973856209\n",
      "\n",
      "Generation 6 - Current best internal CV score: 0.7713071895424836\n",
      "\n",
      "Generation 7 - Current best internal CV score: 0.7741176470588234\n",
      "\n",
      "Generation 8 - Current best internal CV score: 0.7741176470588234\n",
      "\n",
      "Generation 9 - Current best internal CV score: 0.7741176470588234\n",
      "\n",
      "Generation 10 - Current best internal CV score: 0.7761437908496734\n",
      "\n",
      "Generation 11 - Current best internal CV score: 0.7875163398692809\n",
      "\n",
      "Generation 12 - Current best internal CV score: 0.7875163398692809\n",
      "\n",
      "Generation 13 - Current best internal CV score: 0.7875163398692809\n",
      "\n",
      "Generation 14 - Current best internal CV score: 0.7875163398692809\n",
      "\n",
      "Generation 15 - Current best internal CV score: 0.7875163398692809\n",
      "\n",
      "Generation 16 - Current best internal CV score: 0.7875163398692809\n",
      "\n",
      "Generation 17 - Current best internal CV score: 0.7875163398692809\n",
      "\n",
      "Generation 18 - Current best internal CV score: 0.7889542483660129\n",
      "\n",
      "Generation 19 - Current best internal CV score: 0.7889542483660129\n",
      "\n",
      "Generation 20 - Current best internal CV score: 0.7889542483660129\n",
      "\n",
      "Generation 21 - Current best internal CV score: 0.7889542483660129\n",
      "\n",
      "Generation 22 - Current best internal CV score: 0.7889542483660129\n",
      "\n",
      "Generation 23 - Current best internal CV score: 0.7889542483660129\n",
      "\n",
      "Generation 24 - Current best internal CV score: 0.7889542483660129\n",
      "\n",
      "Generation 25 - Current best internal CV score: 0.7889542483660129\n",
      "\n",
      "Generation 26 - Current best internal CV score: 0.7900653594771241\n",
      "\n",
      "Generation 27 - Current best internal CV score: 0.7900653594771241\n",
      "\n",
      "Generation 28 - Current best internal CV score: 0.7900653594771241\n",
      "\n",
      "Generation 29 - Current best internal CV score: 0.7920261437908495\n",
      "\n",
      "Generation 30 - Current best internal CV score: 0.7920261437908495\n",
      "\n",
      "Generation 31 - Current best internal CV score: 0.7920261437908495\n",
      "\n",
      "Generation 32 - Current best internal CV score: 0.7920261437908495\n",
      "\n",
      "Generation 33 - Current best internal CV score: 0.7940522875816994\n",
      "\n",
      "Generation 34 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 35 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 36 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 37 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 38 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 39 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 40 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 41 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 42 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 43 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 44 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 45 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 46 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 47 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 48 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 49 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 50 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 51 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 52 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 53 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 54 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 55 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 56 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 57 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 58 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 59 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 60 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 61 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 62 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 63 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 64 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 65 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 66 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 67 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 68 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 69 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 70 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 71 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 72 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 73 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 74 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 75 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 76 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 77 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 78 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 79 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 80 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 81 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 82 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 83 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 84 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 85 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 86 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 87 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 88 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 89 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 90 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 91 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 92 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 93 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 94 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 95 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 96 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 97 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 98 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 99 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 100 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 101 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 102 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 103 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 104 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 105 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 106 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 107 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 108 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 109 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 110 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 111 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 112 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 113 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 114 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 115 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 116 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 117 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 118 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 119 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 120 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 121 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 122 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 123 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 124 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 125 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 126 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 127 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 128 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 129 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 130 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 131 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 132 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 133 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 134 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 135 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 136 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 137 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 138 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 139 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 140 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 141 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 142 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 143 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 144 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 145 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 146 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 147 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 148 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 149 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 150 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 151 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 152 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 153 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 154 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 155 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 156 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 157 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 158 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 159 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 160 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 161 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 162 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 163 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 164 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 165 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 166 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 167 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 168 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 169 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 170 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 171 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 172 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 173 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 174 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 175 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 176 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 177 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 178 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 179 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 180 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 181 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 182 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 183 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 184 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 185 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 186 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 187 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 188 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 189 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 190 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 191 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 192 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 193 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 194 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 195 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 196 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 197 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 198 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 199 - Current best internal CV score: 0.8050980392156863\n",
      "\n",
      "Generation 200 - Current best internal CV score: 0.8050980392156863\n",
      "                                                                                    \n",
      "Best pipeline: GradientBoostingClassifier(RobustScaler(input_matrix), learning_rate=0.1, max_depth=5, max_features=0.1, min_samples_leaf=4, min_samples_split=12, n_estimators=100, subsample=0.7500000000000001)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from tpot import TPOTClassifier\n",
    "\n",
    "target = \"Cardiac_Category (0=good; 1=poor)\"\n",
    "\n",
    "optimisedFeatureSet = ['C224', 'HistoC162', 'Hips_Circumference__cm', 'HistoC10','HistoC203NDOHC183NDDC', 'HistoC16', 'C222', 'C28', 'Tyr1', 'VO2Max','HistoC143NDOHC123NDDC', 'HistoC143', 'Pulse', 'HistoC121NDOH', 'WHR','C81', 'C101',\"Cardiac_Category (0=good; 1=poor)\"]\n",
    "curatedDf = data_full[optimisedFeatureSet]\n",
    "\n",
    "\n",
    "\n",
    "y = curatedDf[target]\n",
    "curatedDf = curatedDf.drop(target, axis = 1)\n",
    "x = preprocessDf(curatedDf)\n",
    "x = x.fillna(x.median())\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=1)\n",
    "# define search\n",
    "model = TPOTClassifier(generations=200, population_size=50,cv=cv, scoring='accuracy', verbosity=2, random_state=1, n_jobs=2)\n",
    "# perform the search\n",
    "model.fit(x, y)\n",
    "# export the best model\n",
    "model.export('optimisedFeatureSetDf6(Cardiac_Category).py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Preprocessing dataframe...\n",
      "Converting values that are non-numeric AND non-binary to NaN...\n",
      "Number of columns having more than 40 percent missing values:  0\n",
      "These columns are:\n",
      " []\n",
      "Columns with missing values (%):\n",
      "C28    13.95\n",
      "dtype: float64\n",
      "Original Shape:(86, 16)\n",
      "Final Shape:(86, 16)\n",
      "______________________________\n",
      "[0.8  0.2  0.5  0.5  0.75 0.4  0.6  0.75 0.75 0.75 0.6  0.6  0.25 0.75\n",
      " 0.5  0.4  0.6  0.75 0.75 0.75 0.6  0.6  0.5  0.   0.25 0.8  0.8  0.25\n",
      " 0.25 0.5  0.2  0.6  0.5  0.5  0.5  0.4  0.2  0.5  0.25 0.75 0.8  0.4\n",
      " 0.5  0.5  0.75 0.6  0.4  0.25 0.25 0.25]\n",
      "0.512\n",
      "Accuracy: \n",
      "0.5909090909090909\n",
      "Predictions: \n",
      "[0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1]\n",
      "Answers: \n",
      "[0 1 1 0 0 0 1 1 1 0 1 0 0 0 1 1 0 0 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "target = \"Cardiac_Category (0=good; 1=poor)\"\n",
    "\n",
    "selfMadeCols = [\"WaistCircumferencecm\",\"Hips_Circumference__cm\",\"C28\",\"Trp1\",\"HistoC101\",\"HistoC143NDOHC123NDDC\",\"HistoC162\",\"HistoC163\",\"HistoC142\",\"HistoC141\",\"HistoC143\",\"HistoHis1_ConcµM\",\"HistoC225\",\"HistoC203\",\"HistoC182\",\"HistoC181\",\"Cardiac_Category (0=good; 1=poor)\"]\n",
    "curatedDf = data_full[selfMadeCols]\n",
    "\n",
    "\n",
    "#selfMadeCols = data_full[bloodBioCols + [target]]\n",
    "# split into input and output elements\n",
    "\n",
    "\n",
    "y = curatedDf[target]\n",
    "curatedDf = curatedDf.drop(target, axis = 1)\n",
    "x = preprocessDf(curatedDf)\n",
    "xTrain,xTest,yTrain,yTest = train_test_split(x,y,test_size=0.25,random_state=42)\n",
    "\n",
    "xTrain = xTrain.fillna(xTrain.median())\n",
    "xTest = xTest.fillna(xTrain.median())\n",
    "\n",
    "\n",
    "\n",
    "# Average CV score on the training set was: 0.8069716775599128\n",
    "exported_pipeline = make_pipeline(\n",
    "    StackingEstimator(estimator=ExtraTreesClassifier(bootstrap=False, criterion=\"entropy\", max_features=0.35000000000000003, min_samples_leaf=1, min_samples_split=4, n_estimators=100)),\n",
    "    GradientBoostingClassifier(learning_rate=0.01, max_depth=8, max_features=0.55, min_samples_leaf=1, min_samples_split=6, n_estimators=100, subsample=0.6000000000000001)\n",
    ")\n",
    "# Fix random state for all the steps in exported pipeline\n",
    "set_param_recursive(exported_pipeline.steps, 'random_state', 42)\n",
    "\n",
    "#exported_pipeline = GaussianNB().fit(xTrain,yTrain)\n",
    "\n",
    "exported_pipeline.fit(xTrain, yTrain)\n",
    "results = exported_pipeline.predict(xTest)\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=42)\n",
    "CVAccuracyArray = cross_val_score(exported_pipeline,xTest,yTest,cv=cv,scoring='accuracy')\n",
    "MeanCVAccuracyScore = CVAccuracyArray.mean()\n",
    "print(CVAccuracyArray)\n",
    "print(MeanCVAccuracyScore)\n",
    "\n",
    "accuracy = accuracy_score(yTest,results)\n",
    "\n",
    "print(\"Accuracy: \")\n",
    "print(accuracy)\n",
    "\n",
    "print(\"Predictions: \")\n",
    "print(results)\n",
    "print(f\"Answers: \")\n",
    "print(yTest.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}