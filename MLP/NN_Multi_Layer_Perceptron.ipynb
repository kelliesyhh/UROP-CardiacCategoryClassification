{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics of Artificial Neural Networks <a></a>\n",
    "\n",
    "Artificial neural networks (ANN or NN) are computing systems that are inspired by, but not identical to, biological neural networks that constitute animal brains. Such systems learn to perform tasks by considering examples, generally without being programmed with task-specific rules.\n",
    "\n",
    "A NN is based on a collection of connected units or nodes called artificial neurons, which loosely model the neurons in a biological brain. The basic example is the perceptron [1]. Each connection, like the synapses in a biological brain, can transmit a signal to other neurons. An artificial neuron that receives a signal then processes it and can signal neurons connected to it [2].\n",
    "\n",
    "In ANN implementations, the \"signal\" at a connection is a real number, and the output of each neuron is computed by some non-linear function of the sum of its inputs. The connections are called edges. Neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Neurons may have a threshold such that a signal is sent only if the aggregate signal crosses that threshold. Typically, neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the first layer (the input layer), to the last layer (the output layer), possibly after traversing the layers multiple times. [b],\n",
    "\n",
    "![ANN](https://upload.wikimedia.org/wikipedia/commons/4/46/Colored_neural_network.svg)\n",
    "\n",
    "### Single-layer and Multi-layer perceptrons\n",
    "A single layer perceptron (SLP) is a feed-forward network based on a threshold transfer function. SLP is the simplest type of artificial neural networks and can only classify linearly separable cases with a binary target (1, 0). [c], [d]\n",
    "\n",
    "Because SLP is a linear classifier and if the cases are not linearly separable the learning process will never reach a point where all the cases are classified properly. The most famous example of the inability of perceptron to solve problems with linearly non-separable cases is the XOR problem.\n",
    "\n",
    "A multi-layer perceptron (MLP) has the same structure of a single layer perceptron with one or more hidden layers. The backpropagation algorithm consists of two phases: the forward phase where the activations are propagated from the input to the output layer, and the backward phase, where the error between the observed actual and the requested nominal value in the output layer is propagated backwards in order to modify the weights and bias values.\n",
    "\n",
    "-------\n",
    "[1] https://en.wikipedia.org/wiki/Perceptron\n",
    "\n",
    "[2] https://en.wikipedia.org/wiki/Artificial_neural_network\n",
    "\n",
    "[3] https://www.saedsayad.com/artificial_neural_network_bkp.htm\n",
    "\n",
    "[4] https://iamtrask.github.io/2015/07/12/basic-python-network/\n",
    "\n",
    "[5] https://www.freecodecamp.org/news/building-a-neural-network-from-scratch/\n",
    "\n",
    "[6] https://towardsdatascience.com/how-to-build-your-own-neural-network-from-scratch-in-python-68998a08e4f6?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# Required to display matplotlib plots in notebooks\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import itertools "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change directory to your own working directory accordingly\n",
    "currentDir = os.getcwd()\n",
    "\n",
    "pictureDir = os.path.join(currentDir, \"MLP\")\n",
    "pictureDir = os.path.join(pictureDir, \"NN\")\n",
    "if os.path.exists(pictureDir) == False:\n",
    "    os.mkdir(pictureDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Weightkg</th>\n",
       "      <th>Heightcm</th>\n",
       "      <th>Pulse</th>\n",
       "      <th>WaistCircumferencecm</th>\n",
       "      <th>Gender</th>\n",
       "      <th>SBP</th>\n",
       "      <th>DBP</th>\n",
       "      <th>Hips_Circumference__cm</th>\n",
       "      <th>Hypertension__Yes_1_No_0</th>\n",
       "      <th>...</th>\n",
       "      <th>HistoMet1_ConcµM</th>\n",
       "      <th>HistoHis1_ConcµM</th>\n",
       "      <th>HistoPhe1_ConcµM</th>\n",
       "      <th>HistoCit1_ConcµM</th>\n",
       "      <th>HistoTyr1_ConcµM</th>\n",
       "      <th>HistoAsp1_ConcµM</th>\n",
       "      <th>HistoGlu1_ConcµM</th>\n",
       "      <th>HistoTrp1_ConcµM</th>\n",
       "      <th>Cardiac_Category (0=good; 1=poor)</th>\n",
       "      <th>Cardiac_Category EE (0=good; 1=poor)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58.9</td>\n",
       "      <td>157.5</td>\n",
       "      <td>74.5</td>\n",
       "      <td>80.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>145.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>94.971</td>\n",
       "      <td>96.797</td>\n",
       "      <td>40.534</td>\n",
       "      <td>79.3955</td>\n",
       "      <td>49.6495</td>\n",
       "      <td>166.9485</td>\n",
       "      <td>65.5515</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58.9</td>\n",
       "      <td>157.5</td>\n",
       "      <td>74.5</td>\n",
       "      <td>80.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>145.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>94.971</td>\n",
       "      <td>96.797</td>\n",
       "      <td>40.534</td>\n",
       "      <td>79.3955</td>\n",
       "      <td>49.6495</td>\n",
       "      <td>166.9485</td>\n",
       "      <td>65.5515</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>58.9</td>\n",
       "      <td>157.5</td>\n",
       "      <td>74.5</td>\n",
       "      <td>80.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>145.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>94.971</td>\n",
       "      <td>96.797</td>\n",
       "      <td>40.534</td>\n",
       "      <td>79.3955</td>\n",
       "      <td>49.6495</td>\n",
       "      <td>166.9485</td>\n",
       "      <td>65.5515</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>58.9</td>\n",
       "      <td>157.5</td>\n",
       "      <td>74.5</td>\n",
       "      <td>80.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>145.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>94.971</td>\n",
       "      <td>96.797</td>\n",
       "      <td>40.534</td>\n",
       "      <td>79.3955</td>\n",
       "      <td>49.6495</td>\n",
       "      <td>166.9485</td>\n",
       "      <td>65.5515</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 291 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Age  Weightkg  Heightcm  Pulse  WaistCircumferencecm  Gender   SBP   DBP  \\\n",
       "0  58.9     157.5      74.5   80.5                   0.5   145.0  74.0  93.0   \n",
       "1  58.9     157.5      74.5   80.5                   0.5   145.0  74.0  93.0   \n",
       "2  58.9     157.5      74.5   80.5                   0.5   145.0  74.0  93.0   \n",
       "3  58.9     157.5      74.5   80.5                   0.5   145.0  74.0  93.0   \n",
       "\n",
       "   Hips_Circumference__cm  Hypertension__Yes_1_No_0  ...  HistoMet1_ConcµM  \\\n",
       "0                     1.0                       0.0  ...            94.971   \n",
       "1                     1.0                       0.0  ...            94.971   \n",
       "2                     1.0                       0.0  ...            94.971   \n",
       "3                     1.0                       0.0  ...            94.971   \n",
       "\n",
       "   HistoHis1_ConcµM  HistoPhe1_ConcµM  HistoCit1_ConcµM  HistoTyr1_ConcµM  \\\n",
       "0            96.797            40.534           79.3955           49.6495   \n",
       "1            96.797            40.534           79.3955           49.6495   \n",
       "2            96.797            40.534           79.3955           49.6495   \n",
       "3            96.797            40.534           79.3955           49.6495   \n",
       "\n",
       "   HistoAsp1_ConcµM  HistoGlu1_ConcµM  HistoTrp1_ConcµM  \\\n",
       "0          166.9485           65.5515               1.0   \n",
       "1          166.9485           65.5515               1.0   \n",
       "2          166.9485           65.5515               1.0   \n",
       "3          166.9485           65.5515               1.0   \n",
       "\n",
       "   Cardiac_Category (0=good; 1=poor)  Cardiac_Category EE (0=good; 1=poor)  \n",
       "0                                0.0                                  None  \n",
       "1                                0.0                                  None  \n",
       "2                                0.0                                  None  \n",
       "3                                0.0                                  None  \n",
       "\n",
       "[4 rows x 291 columns]"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data.csv\")\n",
    "\n",
    "# Removal of irrelevant columns\n",
    "df.drop('IDshort', axis=1, inplace=True)\n",
    "df.drop('SERNO', axis=1, inplace=True)\n",
    "\n",
    "# Encode categorical values\n",
    "gender = df['Gender'].astype('category')\n",
    "histocode = df['HistoCode'].astype('category')\n",
    "df['Gender'] = gender.cat.codes\n",
    "df['HistoCode'] = histocode.cat.codes\n",
    "\n",
    "# replacing empty data and 'ND' with np.nan and then filling it with the median\n",
    "for colName in df:\n",
    "    df[colName].replace('', np.nan, inplace=True)\n",
    "    df[colName].replace('ND', np.nan, inplace=True)\n",
    "    df[colName] = df.fillna(value=df[colName].median(), inplace=True)\n",
    "\n",
    "df.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Data\n",
    "\n",
    "In our case, we have 8 different sets of features, colour-coded and categorised into 8 categories:\n",
    "1. Clinical Parameters\n",
    "2. Exercise\n",
    "3. Echo Measurements\n",
    "4. CMR Measurements\n",
    "5. Blood Biomarkers\n",
    "6. Physical Function Parameters\n",
    "7. Current Metabolomics\n",
    "8. Historical Metabolomics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting features into different dataframes based on categories\n",
    "X_clinical_params = df.loc[:, 'Age':'AlcoholNever0Current1Past']\n",
    "X_exercise = df.loc[:, 'PhysicalactivityfrequencyIna':'VO2Max']\n",
    "X_echo_measurements = df.loc[:, 'BSA__m2':'E_Eprime_ratio']\n",
    "X_cmr_measurements = df.loc[:, 'LV_Mass_on_mri__g':'LVSVImlm2']\n",
    "X_blood_biomarkers = df.loc[:, 'MCP1pgmL':'HbA1c']\n",
    "X_physical_func_params = df.loc[:, 'Gripmax': 'ALM']\n",
    "X_curr_metabolomics = df.loc[:, 'TC':'Trp1']\n",
    "X_hist_metabolomics = df.loc[:, 'HistoC2':'HistoTrp1_ConcµM']\n",
    "\n",
    "features = [X_clinical_params, X_exercise, X_echo_measurements, X_cmr_measurements, X_blood_biomarkers, X_physical_func_params, X_curr_metabolomics, X_hist_metabolomics]\n",
    "\n",
    "# Cardiac outcomes\n",
    "y1 = df['Cardiac_Category (0=good; 1=poor)']\n",
    "y2 = df['Cardiac_Category EE (0=good; 1=poor)']\n",
    "\n",
    "labels = [y1, y2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron\n",
    "\n",
    "The perceptron is a basic function that mimics the human neuron. It receives $n$ inputs, associated to the dendrites inputs to the neuron. Each dendrite, due to *lernging*, is weighted by a number that signals its input relevance for the neuron [1]. \n",
    "\n",
    "![Neuron](https://upload.wikimedia.org/wikipedia/commons/a/a9/Complete_neuron_cell_diagram_en.svg)\n",
    "\n",
    "The signal is thus elaborated and passed through the *axon* to others neurons [2]; actually, the neurons *fires* the signal only if the elaborated inputs have surpassed a certain threshold; this is a spiking neuron [3].\n",
    "\n",
    "The perceptron wants to mimic it. Receinving a vector (i.e. array) $x_i$ of signals, where $i$ stands for the $i$-th dendrites, it weights each of them by a vector of weights $w_i$. It adds also a *bias* to remove near-zero issues (the bias shifts the decision boundary away from the origin and does not depend on any input value).\n",
    "\n",
    "![Perceptron](https://miro.medium.com/max/2870/1*n6sJ4yZQzwKL9wnF5wnVNg.png)\n",
    "\n",
    "### Activation functions\n",
    "\n",
    "Also, the perceptron ignite an output through an activation function that is usually a *sigmoid* function [4]\n",
    "$$f (x) = \\frac{1}{1+e^{-x}} \\,,$$\n",
    "\n",
    "![sigmoid](https://upload.wikimedia.org/wikipedia/commons/8/88/Logistic-curve.svg)\n",
    "\n",
    "or a *rectifier*\n",
    "$$\\varphi(x) = \\mathrm{max}[0, x] \\,,$$\n",
    "![ReLU](https://upload.wikimedia.org/wikipedia/commons/6/6c/Rectifier_and_softplus_functions.svg)\n",
    "\n",
    "so that the output $O(x_i)$ of the perceptron is given by\n",
    "\n",
    "$$O (x_i) = \\varphi \\left( \\Sigma_{i=1}^{n} w_i \\, x_i + b   \\right) ,$$\n",
    "\n",
    "or, in vectorial representation \n",
    "\n",
    "$$O(x) = \\varphi \\left(\\mathbf{w}^T \\cdot \\mathbf{x} + b   \\right) $$\n",
    "\n",
    "Below we present a simple code implementation of the perceptron.\n",
    "\n",
    "-----\n",
    "[1] https://en.wikipedia.org/wiki/Dendrite\n",
    "\n",
    "[2] https://en.wikipedia.org/wiki/Neuron\n",
    "\n",
    "[3] https://icwww.epfl.ch/~gerstner/BUCH.html\n",
    "\n",
    "[4] https://en.wikipedia.org/wiki/Rectifier_(neural_networks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# Define the sigmoid activator; we ask if we want the sigmoid or its derivative\n",
    "def sigmoid_act(x, der=False):\n",
    "    if der: #derivative of the sigmoid\n",
    "        f = x/(1-x)\n",
    "    else: # sigmoid\n",
    "        f = 1/(1+ np.exp(-x))\n",
    "    return f\n",
    "\n",
    "# We may employ the Rectifier Linear Unit (ReLU)\n",
    "def ReLU_act(x, der=False):\n",
    "    if der:\n",
    "        if x > 0 :\n",
    "            f = 1\n",
    "        else :\n",
    "            f = 0\n",
    "    else :\n",
    "        if x > 0:\n",
    "            f = x\n",
    "        else :\n",
    "            f = 0\n",
    "    return f\n",
    "\n",
    "# Now we are ready to define the perceptron; \n",
    "# it eats a np.array (that may be a list of features )\n",
    "def perceptron(X, act='Sigmoid'):     \n",
    "    shapes = X.shape # Pick the number of (rows, columns)!\n",
    "    n= shapes[0] + shapes[1]\n",
    "    # Generating random weights and bias\n",
    "    w = 2*np.random.random(shapes) - 0.5 # We want w to be between -1 and 1\n",
    "    b = np.random.random(1)\n",
    "    \n",
    "    # Initialize the function\n",
    "    f = b[0]\n",
    "    for i in range(0, X.shape[0]-1) : # run over column elements\n",
    "        for j in range(0, X.shape[1]-1) : # run over rows elements\n",
    "            f += w[i, j]*X[i,j]/n\n",
    "    # Pass it to the activation function and return it as an output\n",
    "    if act == 'Sigmoid':\n",
    "        output = sigmoid_act(f)\n",
    "    else :\n",
    "        output = ReLU_act(f)\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of an output of the Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "(0, 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2645\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2646\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2647\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: (0, 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-351-1e9885f78582>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_no\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Output with sigmoid activator: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperceptron\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Output with ReLU activator: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperceptron\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-350-081836bf85b1>\u001b[0m in \u001b[0;36mperceptron\u001b[0;34m(X, act)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;31m# run over column elements\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;31m# run over rows elements\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0;31m# Pass it to the activation function and return it as an output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mact\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Sigmoid'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2798\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2799\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2800\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2801\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2802\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2646\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2647\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2648\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2649\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2650\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: (0, 0)"
     ]
    }
   ],
   "source": [
    "# Converting features and labels to a 2 column matrix and getting both activator\n",
    "feature_no = 1\n",
    "np_features = []\n",
    "np_labels = []\n",
    "\n",
    "for X in features:\n",
    "    print(feature_no)\n",
    "    print('Output with sigmoid activator: ', perceptron(X))\n",
    "    print('Output with ReLU activator: ', perceptron(X))\n",
    "    print(\"\\n\")\n",
    "    feature_no += 1\n",
    "    \n",
    "for y in labels:\n",
    "    y = y.to_numpy()\n",
    "    np_labels.append(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network's Layer(s)\n",
    "\n",
    "A standard Artificial Neural Network will be made of multiple layers:\n",
    "1. An **Input Layer**, that pass the features to the NN\n",
    "2. An arbitrary number of **Hidden Layers**, containing an arbitrary number of neurons for each layer, that receives the inputs and elaborate them. We will introduce Hidden Layers with ReLU activator, since in the *hidden* part of the NN we don't need the output to be contained in the $[0,1]$ range. \n",
    "3. An **Output Layer**: these layers contains a number of neurons equal to the number of possible labels we want to have a prediction to; this is because the output of the NN is thus a vector whose dimension is the same as the cardinality of the set of labels, and its entries are the *probability* for each label for the element whose feateures we have passed to the NN. This means that we will use a sigmoid activator to the Output layer, so we squeeze each perceptron's output between 0 and 1. \n",
    "\n",
    "![ANN](https://miro.medium.com/proxy/1*DW0Ccmj1hZ0OvSXi7Kz5MQ.jpeg)\n",
    "\n",
    "In this case, since we have a binary classification (Survived/Perished) we may simply use a single-perceptron Output layer; If the output is smaller than 0.5, the person is perished; otherwise, the person is survived. \n",
    "\n",
    "For each layer, we have as an input a matrix made by columns of features (in our example, we have 2 features, i.e. Passenger Class and Passenger Sex), that we label as $I=1,2$. Each of this features will have $n$ entries, so that each feature is a vector $\\{x_I\\}_i$. The layer will have $p$ perceptrons, labelled by $a=1,\\ldots ,p$. Thus the output of the whole layer is a matrix ${O}^{(a)}_{(I)}$ given by\n",
    "$${O}^{(a)}_{(I)} = \\varphi\\left( \\mathbf{w}^{(a)}_{(I)} \\cdot  \\mathbf{x}^{(a)}_{(I)} + b^{(a)}_{(I)}   \\right) ,$$\n",
    "\n",
    "or, explicitly \n",
    "$${O}^{(a)}_{(I)} = \\varphi ( {\\large \\Sigma}_{i=1}^{n} \\left({w}^{(a)}_{(I)}\\right)_i \\left(  {x}^{(a)}_{(I)} \\right)_i + \\left( b^{(a)}_{(I)} \\right)_i  )  .$$\n",
    "\n",
    "If we start having multiple layers, let us say $N$, we have an additional label $A=1, \\ldots, N$ so that \n",
    "$${}^{(A)}{O}^{(a)}_{(I)} = {}^{(A)}\\varphi\\left( {}^{(A)}\\mathbf{w}^{(a)}_{(I)} \\cdot  {}^{(A)}\\mathbf{x}^{(a)}_{(I)} + {}^{(A)}b^{(a)}_{(I)}   \\right) ,$$\n",
    "\n",
    "where we have inserted the label also to the activation function that may depend on which layer we are considering!\n",
    "\n",
    "## Backpropagation and Gradient Descent\n",
    "\n",
    "For adjusting the trainable parameters $\\{w\\}$ and $\\{b\\}$, we need to implement the *backpropagation*. We want to minimize a certain **cost function**\n",
    "$$\\mu(y,\\bar{y})=|y-\\bar{y}|^2$$\n",
    "where $y$ is the output of the output layer while $\\bar{y}$ is the actual label; in order to do so, we start the *gradient descent*, which means that we see the cost function as a function of the trainable parameters $\\mathbf{w}$ such as $\\{w\\}$ and $\\{b\\}$, we compute the gradient - gradient that can be seen as the slope of the multidimensional graph [3] - and we subtract it from the randomly initialized set, as\n",
    "$$\\mathbf{w}'_n =  \\mathbf{w}_n - \\eta \\nabla \\mu(\\mathbf{w}_n)  \\,,$$\n",
    "moving thus towards the *optima*, or global minimum, of the cost function. In the formula above $\\eta$ is the **learning rate** of the ANN [4] \n",
    "\n",
    "\n",
    "This is pictorially represented in the following picture [5]:\n",
    "\n",
    "![GD](https://hackernoon.com/hn-images/1*f9a162GhpMbiTVTAua_lLQ.png)\n",
    "\n",
    "**NB:** If you didn't want to commit yourself to the math, you can just jump to the **Our ANN** section below;\n",
    "\n",
    "Explicitly, the $\\alpha$-th hidden layer is defined by a *matrix* $w_{\\alpha}^{i_{\\alpha} i_{\\alpha-1}}$ and a vector $b_\\alpha^{i_\\alpha}$, and its output is\n",
    "$$z_\\alpha^{i_\\alpha} = \\varphi ( w_{\\alpha}^{i_{\\alpha} i_{\\alpha-1}} \\, z^{i_{\\alpha-1}}_{\\alpha-1} + b_\\alpha^{i_\\alpha} ) \\equiv \\varphi (\\zeta^{i_\\alpha}_\\alpha)  .$$\n",
    "\n",
    "The output layer, on the contrary, has a binary output with sigmoid activation function\n",
    "$$y = f( w_{Out}^{i_n} z_n^{i_n} + b_{Out} ) \\equiv f (\\gamma) .$$\n",
    "\n",
    "Notice that we have used the greek letter to define the value of the output before employing the activation function, i.e. the greek letters refer to the output of the *soma*, while the latin letter are the output of the *axon*.\n",
    "\n",
    "We need thus to compute the gradient $\\nabla \\mu$, where each element is $\\partial \\mu / \\partial z_\\alpha^{i_\\alpha}$; by applying the chain rule repeaditly we get \n",
    "$$ \\frac{\\partial \\mu}{\\partial z_{\\alpha}^{i_\\alpha}} = \\frac{\\partial \\mu}{\\partial z_{\\alpha+1}^{i_{\\alpha+1}} } \\cdot \\frac{\\partial z_{\\alpha+1}^{i_{\\alpha+1}} }{\\partial z_{\\alpha}^{i_\\alpha}} = \\frac{\\partial \\mu}{\\partial z_{\\alpha+1}^{i_{\\alpha+1}} } \\cdot w_{\\alpha}^{i_{\\alpha+1} i_\\alpha} \\varphi' (z_{\\alpha}^{i_\\alpha}) \\,.$$\n",
    "\n",
    "Defininf **the error** for the $\\alpha$-th layer as \n",
    "$$\\delta_\\alpha^{i_\\alpha} = \\frac{\\partial \\mu}{\\partial z_{\\alpha}^{i_\\alpha}}$$\n",
    "we see that, working *backwards*, we may obtain it from the previously computed $(\\alpha+1)$-th error via [6]\n",
    "$$\\delta_{\\alpha}^{i_\\alpha}  = {\\large \\Sigma}_{{i_\\alpha}}  \\delta_{\\alpha+1}^{i_\\alpha+1}  \\, w_{\\alpha}^{i_{\\alpha+1} i_\\alpha} \\, \\varphi' (z_{\\alpha}^{i_\\alpha}) \\,.$$\n",
    "\n",
    "This means that working iteratively, starting from the Output layer, we may easly compute the errors. \n",
    "\n",
    "Now, the goal of this whole procedure is to go towards a global minimum (or *optima*) of $\\mu$, seen as a function of the parametes $w_\\alpha^{i_{\\alpha+1} i_\\alpha}$ and $b_\\alpha^{i_{\\alpha}}$; \n",
    "\n",
    "So, after having shifted the parameters as dictated by our gradient descent equation, we need to compute the variation of the cost function under the variation of parameters, to see if we have moved toward a decreasing cost-function path; in order to do so, we apply again the chain rule\n",
    "$$\\frac{\\partial \\mu}{\\partial w_\\alpha^{i_{\\alpha+1} i_\\alpha} } = \\frac{\\partial \\mu}{\\partial z_{\\alpha}^{i_\\alpha}}  \\cdot \\frac{\\partial z_\\alpha^{i_\\alpha} }{\\partial w_\\alpha^{i_{\\alpha+1} i_\\alpha} }  = \\delta_\\alpha^{i_\\alpha} \\cdot z_{\\alpha-1}^{i_\\alpha-1} \\varphi'(\\zeta_\\alpha^{i_\\alpha} ) ,$$\n",
    "\n",
    "$$\\frac{\\partial \\mu}{\\partial b_\\alpha^{i_{\\alpha}} } =  \\frac{\\partial \\mu}{\\partial z_{\\alpha}^{i_\\alpha}}  \\cdot \\frac{\\partial z_\\alpha^{i_\\alpha} }{\\partial b_\\alpha^{i_{\\alpha}} } =  \\delta_\\alpha^{i_\\alpha} \\cdot  \\varphi'(\\zeta_\\alpha^{i_\\alpha} ) . $$\n",
    "\n",
    "\n",
    "### TL;DR: (a.k.a: recap)\n",
    "\n",
    "We now recap what we have shown: \n",
    "* **Input**: Set up the inputs $z_0^{i_0}$;\n",
    "* **Feed Forward**: Computes the output of the $\\alpha$-th layer $z_\\alpha^{i_\\alpha}$ via the formula \n",
    "$$z_\\alpha^{i_\\alpha} = \\varphi ( w_{\\alpha}^{i_{\\alpha} i_{\\alpha-1}} \\, z^{i_{\\alpha-1}}_{\\alpha-1} + b_\\alpha^{i_\\alpha} ) \\equiv \\varphi (\\zeta^{i_\\alpha}_\\alpha) ,$$\n",
    "up to the output layer, where the formula is \n",
    "$$y = f( w_{Out}^{i_n} z_n^{i_n} + b_{Out} ) .$$\n",
    "* **Compute the errors**: compute the error of the last layer via the formula \n",
    "$$\\delta^N_{i_N} = \\frac{\\partial \\mu}{\\partial z_N^{i_N} } \\,.$$\n",
    "In our case the last Layer is actually the output layer (that we trat differently), so we have\n",
    "$$\\delta^{Out} = \\frac{\\partial \\mu}{\\partial z_{Out} } \\cdot f' (z_{Out}) .$$\n",
    "*  **Backpropagate the Error**: For each layer $\\alpha= N-1, \\ldots, 2$ compute \n",
    "$$\\delta_{\\alpha}^{i_\\alpha}  = {\\large \\Sigma}_{{i_\\alpha}}  \\delta_{\\alpha+1}^{i_\\alpha+1}  \\, w_{\\alpha}^{i_{\\alpha+1} i_\\alpha} \\, \\varphi' (z_{\\alpha}^{i_\\alpha}) \\,.$$\n",
    "* **Output**: the gradient of the cost function is now computed by the formulae\n",
    "$$\\frac{\\partial \\mu}{\\partial w_\\alpha^{i_{\\alpha+1} i_\\alpha} } = \\delta_\\alpha^{i_\\alpha} \\cdot z_{\\alpha-1}^{i_\\alpha-1} \\varphi'(\\zeta_\\alpha^{i_\\alpha} ) ,$$\n",
    "$$\\frac{\\partial \\mu}{\\partial b_\\alpha^{i_{\\alpha}} } =  \\delta_\\alpha^{i_\\alpha} \\cdot  \\varphi'(\\zeta_\\alpha^{i_\\alpha} ) . $$\n",
    "\n",
    "Notice that the explicit derivatives of the activation functions are know and easy to compute, since\n",
    "$$f'(x) = f(x)(1-f(x)) = y(1-y) \\,,$$\n",
    "while \n",
    "$$\\varphi'(x) = \\Theta(x)\\,,$$\n",
    "where $\\Theta(x)$ is the *Heaviside Theta*, that outputs 1 for $x\\ge 0$ and $0$ otherwise. \n",
    "\n",
    "We need to build now an algorithm that will perform the previous task in the followin manner: \n",
    "1. **Input a set of training examples** or *batches*.\n",
    "2. **For each training batch** $x_{i_0}$: set the corresponding input activation $z_1^{i_1} = \\varphi(w_1^{i_1 i_0} x_{i_0} + b_1^{i_1})$, and then perform the following steps: \n",
    "    1. **Feedforward**: $\\forall \\alpha=2,\\ldots N$ compute \n",
    "    $$z_\\alpha^{i_\\alpha} = \\varphi ( w_{\\alpha}^{i_{\\alpha} i_{\\alpha-1}} \\, z^{i_{\\alpha-1}}_{\\alpha-1} + b_\\alpha^{i_\\alpha} ) \\equiv \\varphi (\\zeta^{i_\\alpha}_\\alpha) ,$$\n",
    "    $$y = f( w_{Out}^{i_n} z_n^{i_n} + b_{Out} ) $$\n",
    "    2. **Compute the outer layer Error**: Compute the vector\n",
    "    $$\\delta^{Out} = \\frac{\\partial \\mu}{\\partial z_{Out} } \\cdot f' (z_{Out}) .$$\n",
    "    3. **Backpropagate the error**: for each $\\alpha = N, N-1, \\ldots 2$ compute \n",
    "    $$\\delta_{\\alpha}^{i_\\alpha}  = {\\large \\Sigma}_{{i_\\alpha}}  \\delta_{\\alpha+1}^{i_\\alpha+1}  \\, w_{\\alpha}^{i_{\\alpha+1} i_\\alpha} \\, \\varphi' (z_{\\alpha}^{i_\\alpha}) \\,.$$\n",
    "3. **Gradient Descent**: for each $\\alpha = N, N-1, \\ldots 2$ update the weights $\\{ w_\\alpha^{i_\\alpha i_{\\alpha-1} } \\,, b_\\alpha^{i_\\alpha} \\}$ via the rule\n",
    "$$w_\\alpha^{i_\\alpha i_{\\alpha-1} } \\mapsto w_\\alpha^{i_\\alpha i_{\\alpha-1} } - \\eta {\\large \\Sigma}_{i_\\alpha} \\delta_\\alpha^{i_\\alpha} \\cdot z_{\\alpha-1}^{i_{\\alpha-1}} \\,,$$\n",
    "$$b_\\alpha^{i_\\alpha  } \\mapsto b_\\alpha^{i_\\alpha  } - \\eta {\\large \\Sigma}_{i_\\alpha} \\delta_\\alpha^{i_\\alpha} \\,.$$\n",
    "\n",
    "## Our ANN\n",
    " \n",
    "Let us work out the example for our Neural Network: we will initialize an Artificial Neural Network having\n",
    "1. An **input Layer** whose inputs is a 2-entris vector $x_i$ = $[$ Passenger Class, Sex $]$. \n",
    "2. **Two Hidden Layers** with respectively $p$ and $q$ neurons *each*, with ReLU activator $\\varphi$, whose outpus are $z_1^{j}$ and $z_2^{k}$ and whose parameters are $\\{ w_1^{j i}, b_1^j\\}$ and $\\{ w_2^{kj}, b_2^k\\}$;\n",
    "3. **A single-neuron output layer**, since we want to have a binary classification, that has a sigmoid activator $f$ and outputs a $y\\in [0,1]$ real number and whose parameters are $\\{ w_{Out}^k, b_{Out}\\}$; \n",
    "\n",
    "This ANN look somewhat similar to the picture below.\n",
    "\n",
    "![HL](http://cs231n.github.io/assets/nn1/neural_net2.jpeg)\n",
    "\n",
    "This means that our algorithms will have to perform the following steps:\n",
    "0. After a train/test split of both training data $\\{x_i\\}_I$ and labels $\\{\\bar{y}\\}_I$, where $I$ runs over the passengers, it has to reassamble the train data into batches to train our Neural Network. For sake of simplicity, and since we are employing *batch gradient descent* [7], we will use the whole training dataset as a batch and thus have only 1 epoch. \n",
    "1. Inputs the training data $\\{x_i\\}_I$ and feed them to the first Hidden Layer;\n",
    "2. Initialize randomly all the parameters for each layers, i.e. $\\{ w_1^{j i}, b_1^j\\}$, $\\{ w_2^{kj}, b_2^k\\}$ and $\\{ w_{Out}^k, b_{Out}\\}$. Notice that $i=0,1$, $j=0, \\ldots, p-1$ and $k=0, \\ldots, q-1$. Now we can perform the following steps: For each $I$ in the passenger list, we have to\n",
    "    1. **Feedforward**: compute *in this order*\n",
    "    $$z_1^j = \\varphi( {\\large \\Sigma}_{i=0,1} \\, w_1^{j i} x^i + b_1^j ), $$\n",
    "    $$z_2^k = \\varphi( {\\large \\Sigma}_{j=0,\\ldots, p-1} \\, w_2^{k j} z_1^j + b_2^k ), $$\n",
    "    $$y = f( {\\large \\Sigma}_{k=0, \\ldots, q-1} w_{Out}^k z_2^k + b_{Out} ) .$$\n",
    "    2.  **Compute the outer layer Error**: Compute the vector\n",
    "    $$\\delta_{Out} = \\frac{\\partial \\mu}{\\partial y} \\cdot f' (y) = 2(y-\\bar{y}) \\cdot y(1-y) $$\n",
    "    3. **Backpropagate the error**: Compute *in this order* the following\n",
    "    $$\\delta_2^{k} =   \\delta_{Out}  \\, w_{Out}^{k} \\, \\varphi' (z_{2}^{k}) ,$$\n",
    "    $$\\delta_1^j = {\\large \\Sigma}_{k=0, \\ldots, q-1} \\delta_2^{k} w_2^{k j}  \\, \\varphi' (z_{1}^{j}) . $$\n",
    "3. **Gradient descent**: When we pass from the $I$-th to the $(I+1)$-th passenger in the training set, we need to update the weights *in this order* following the rules\n",
    "    * Output Layer:\n",
    "    $${}^{(I+1)}w_{Out}^k = {}^{(I)} w_{Out}^k - \\eta {}^{(I)}\\delta_{Out} \\, z_2^k \\,,$$\n",
    "    $${}^{(I+1)}b_{Out} = {}^{(I)} b_{Out} - \\eta {}^{(I)}\\delta_{Out}\\,,$$\n",
    "    * Second Layer:\n",
    "    $${}^{(I+1)}w_{2}^{kj} = {}^{(I)} w_{2}^{kj} - \\eta \\delta_2^{k} \\, z_1^{j} \\,,$$\n",
    "    $${}^{(I+1)}b_{2}^k = {}^{(I)} b_{2}^k - \\eta \\delta_2^{k} \\,, $$\n",
    "    * First Layer:\n",
    "    $${}^{(I+1)}w_{1}^{ji} = {}^{(I)} w_{1}^{ji} - \\eta \\delta_1^{j} \\, x_i^{i} \\,,$$\n",
    "    $${}^{(I+1)}b_{1} = {}^{(I)} b_{1}^j - \\eta \\delta_2^{j} \\,, $$\n",
    "\n",
    "Now we can iterate over the whole set $I=0, \\ldots \\, \\#$training_set_passengers so that we can **train** our Neural Network!\n",
    "\n",
    "-------\n",
    "[1] http://cs231n.github.io/neural-networks-1/\n",
    "\n",
    "[2] https://machinelearningmastery.com/implement-backpropagation-algorithm-scratch-python/\n",
    "\n",
    "[3] https://www.nature.com/articles/323533a0 (original paper) https://en.wikipedia.org/wiki/Gradient_descent (Wiki article)\n",
    "\n",
    "[4] https://en.wikipedia.org/wiki/Stochastic_gradient_descent\n",
    "\n",
    "[5] https://hackernoon.com/gradient-descent-aynk-7cbe95a778da\n",
    "\n",
    "[6] http://neuralnetworksanddeeplearning.com/chap2.html\n",
    "\n",
    "[7] https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/\n",
    "\n",
    "[Picture 1] https://medium.com/datadriveninvestor/when-not-to-use-neural-networks-89fb50622429"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sigmoid activator; we ask if we want the sigmoid or its derivative\n",
    "def sigmoid_act(x, der=False):\n",
    "    if der: #derivative of the sigmoid\n",
    "        f = 1/(1+ np.exp(- x))*(1-1/(1+ np.exp(- x)))\n",
    "    else: # sigmoid\n",
    "        f = 1/(1+ np.exp(- x))\n",
    "    return f\n",
    "\n",
    "# We may employ the Rectifier Linear Unit (ReLU)\n",
    "def ReLU_act(x, der=False):\n",
    "    if der: # the derivative of the ReLU is the Heaviside Theta\n",
    "        f = np.heaviside(x, 1)\n",
    "    else :\n",
    "        f = np.maximum(x, 0)\n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test split\n",
    "\n",
    "We now split the set of features and labels into a training set and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-354-cb821399be9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# labels[1] corresponds to \"Cardiac_Category EE (0=good; 1=poor)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training records:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# split into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# labels[0] corresponds to \"Cardiac_Category (0=good; 1=poor)\"\n",
    "# labels[1] corresponds to \"Cardiac_Category EE (0=good; 1=poor)\"\n",
    "for X in features:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X.to_numpy(), np_labels[0], test_size=0.25)\n",
    "\n",
    "print('Training records:', y_train.size)\n",
    "print('Test records:', y_test.size)\n",
    "\n",
    "# X_train = X_train.to_numpy()\n",
    "# y_train = y_train.to_numpy()\n",
    "# X_test = X_test.to_numpy()\n",
    "# y_test = y_test.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-293-15575d3ca699>:6: RuntimeWarning: overflow encountered in exp\n",
      "  f = 1/(1+ np.exp(- x))\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnUAAAGNCAYAAAB33oe9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3debhkVX3u8e9rA84RlGYQ0IbYiRITImlb1Kg4YJiumEmBqwKJ4hj1GgfUaxySOMcpTiFRARGJRqPEtBdxADQ40CCiiEhLVCahAcEBEFt+94+9TyyKOt11uk+fKtb5fp5nP1V77bX3XqvW6T7v2VOlqpAkSdJt2+0m3QBJkiRtOkOdJElSAwx1kiRJDTDUSZIkNcBQJ0mS1ABDnSRJUgMMdZJ0G5Wkkpw66XbcFiU5pv/8lk26LdJ8MdRJUyzJy/tfPJXktyfdnlbMfKaTbockzSdDnTSlkgT4S2AmfDxtgs2RJE05Q500vR4L7AocC1wBHJZkq8k2SZI0rQx10vSaOTL3z8CHgG2BPx6skOTk/lTiHqM2kOTgfvmbhsrvnuR1Sc5PckOS65J8LsljR2zj8H4bhyfZN8mpff0aqPP4JMcn+W6Snyf5WZKzkjw3ycj/Z5L8VpKPJflxv84ZSQ4Y3N+IdXZO8s4kFyX5RZKrk5yU5IEb+Cw3WpLbJzkqyblJrk/ykyRfTPKEWeo/rv8sL+/beFmS05I8a6jebkmOTrKmH4NrknwzyXuT3GOObbxnkg8mubLf1llJDh2qs2//ub5/Pf28qp9uP+Z+79tfm3Zx39crkpww6lKBgWvYdkvygiTfSXJjkkuSvDXJb8yyjz/of06u7PfxgyTvTrLjLPXvlOQlSVYn+Wn/s3h+knck2X6WdZ7ef/Y39n04OsndxvkMpKlSVU5OTlM2AdsDNwEX9PP3pzsN+7mheof05f8wy3ZW9cvvP1B2b+C/+/LTgbcCRwOXATcDTxvaxuF93U8B64D/AN4A/OtAne8A3wY+CLweeA9wQb/eB0e0677A1QPbfS1wYt/nT/Tlhw+tsydwVd/GTwNvBo4BrgV+Aew/h8+3uv/+NlhvK+DUvv75wJuAd9EdOS3gtUP1j+zLL+8/09cC/wJ8DThzoN6Off9/CXyy/zzfDpwE/HxwvMboxzeA7wPn9Nv5J+DH/bIXDdQNsKbf/t1GbOvQfp03j7nvfYHr+z58HHgjcAJwI3AdsOdQ/WP67X+yb98/9e09py9fDdxhaJ0D+7G9qd/264DP9PUvBZYN1d9mYHvf6T/TN/Xt+xmw94j2fKRv7/HAPwBn9+Wfn/T/A05Oc50m3gAnJ6dbT8BR/S+Wlw6UndUHmvsMlN2hDzU/ArYY2sYOdCHsrKHyU/vtHDxUvnX/C/EGYPuB8sP7ttwM7DtLe39zRNnt6E4dF/CgoWWf68ufOVS+X19+i1AHbNEHkhuBRwytc8/+F/zlwO3H/HzHDXUv7euuGvx8ge3oglQBDxkao18A243Y1rYD7/+qX/d5I+rdGbjjXPrRB5PbDZTvClzTh6HdBspf2Nd/zohtndov+60x9rsNXTC7Cth9aNnv9AHq7KHymRB1FXDvoZ+Tj/XLXjFQfpe+7q+Ahw1t6yV9/c8MlZ/Ql79n8PPol92VgTA70J4fAvca+lk7vV+2ci7/bp2cJj15+lWaMv0NEk+lC1HHDSw6hu5oy1NnCqrqRrpf6NsDfzS0qScBS+iC1cy29wAeAXysqk4crFxV1wKvpAuKfzqiaZ+sqv83qs1V9b0RZTfTHSlhsG1JdgEeRRfS/mlonU8Dnx2xiwOA3wT+sapOG1rnMrqjRDsAjx7Vvk3wF3S/3F9QVesG9nkl8Lf97FOH1llHd/TqFqrqqhHbv2FEvZ9X1a3K1+NXwEv6z3tmG/8NvAPYEnjyQN0P0AXjpw9uoD9d+gjgC1X13TH2+RS6PwJeWVXfHmr/eXSXDDwgye4j1n17Vf1goP7NwIvoft7/YqDeQcA96I4If3FoG/9AF6r3SXKvvg/bAU+kC/cvHPw8+v38tKquG9Ge11TVDwfqraP7nABWjqgvTa0tJt0ASbfyKLoAc3JVXTpQfgLdKcfDk7yiqmaCwzF0198dBvznQP3D6MLFCQNlD+5f75bkVSP2vbR/vd+IZV+brcH9NWAvAvYHdqM72jRop4H3v9+/fnn4F2/vS8Bjhspm2n3vWdq9vH+9H91RtU2W5K7AfYBLq+o7I6p8vn99wEDZh+gCx3lJ/hU4Dfivqlo7tO5JdKdm35Xkj4CTgf8Cvl1Vc33Uyg/7EDfsVLqQ/j/tq6qrk3wEeEqSh1TVGf2iI/vX9465z5nx2GOW8fit/vV+dKflB502NE9VXZTkYmBZkq37PzD27Bd/fkT9dUlOB5bR9e+HwAPpjvqdXlU/H7Mf0J32HXZx/7rNHLYjTZyhTpo+M79gjxks7H8h/wfdUbSDgH/ry89I8l3gcUm2qaofJ9mT7jq8TwwdIZq5AH+ffprNXUaU/WhUxSRbA2fSnfL7Gt3RxWvojlhtDTwPGLzwfuYC9Ctm2feo8pl2//l62gyj272xZtp5+SzLZ8q3nimoqrckuQp4FvBc4PlAJTmN7vq21X29HyRZCbyK7tq0P+k3cXGSN1fVO+bQztk+x5nxGr7g/910R9qeDpzR3xRxGHAl3fWM45gZjw09ZmfUeKyvvfema++1zP3zn3m9dETd9bl2RNnMUdklc9yWNFGefpWmSJKlwOP72Q/n1w8ennlY7sxp0SOHVj2OLjg9sZ8/rH89dqjezOmn51VV1jMdMaJ5sx1BeipdoHt1VT2oqp5VVf+3ql4F/OuI+j/pX0feiThL+Uy7D9pAu189yzY3xsw+d5hl+Y5D9QCoquOqai+64HMA8D7g4cDJ/SnCmXrnV9UT+3or6K6jvB3w9iR/OYd2zvY5zrR7uH1fpbsZ4AlJtqH7mboH8IGqumnMfc5sc48NjMfwz99c2jvXz38mnO00oq60KBjqpOlyGN0dl2fRhYFR01rgMUl2HVjvOLprkg5LsiXdXbFXccvTsQBf6V8fNo9tvk//+rERyx4xouzr/euDM/pxJ384omxztHu9quqnwPeAnZIsH1Hlkf3r2bOsf21Vraqqp9Eddb07I9pfVeuq6qyqegPduMGvg/047pXRX3W1d//69RHL3kN37eRT+PUdu/88h31uynjc6mciyW7ALsD3+1Ov8Ot27z2i/hb8+udk5vP/Gt2/gYcnGT79Ly0KhjppusxcdP+sqnrqqInu5oLhGyYuprv2aC+6051LgRMGrrubqbca+CLwJ0kGL0r/H0l+d/CI0hi+37/uPbSdB9DdPXoLfVtPpQuDwxfs78utr6eD7jEY3wOenWT/Wdr94CR3mkO7x/F+us/6TUn+51Rckm2BVwzUmSnftw8cw2Y+z+v7eitneWba9oP1xrQEeMNgQO4D/3PpTiMeP2KdE+iOcL2YLmSdMupml/X4AN2RsVf2p5FvIcntkuw9y7rPS3Lvwbp0jx25Hb++QQG6U8HXAIck2WtoG8+nu3bzszM3OfTXLZ5IdwTvzcN/MCS5i8+eU+u8pk6aEv0vwd8GvllVs96UQHe07uXAEUleOXBX5rF0gei1A/OjHEoXAN+X5LnAV+l+Qe8M/B7dtXgPprvGahzH0d0k8bYkjwQupLtx4UC654M9ccQ6z6a7MeDdfUg7l+6X9J/SBbiD6I66AFBVv0zyJ3Q3FPxnkjPoHr9yPd0Rngf26+/IHAJRkmPWs/hZdDem7Ne35xtJVgF3oru2bzvgjVX1pYF1TgRuTPIlurAbuqNZD6Q7+jpzZ++hdAH1NLq7gH9Md3PM/6J7JMrbxu0D3Wf3IOCsJJ+huxbtiXTXmL14ljuTr09yLF3wg6G7kDekv77zz4B/B76S5HPAeXRjdi+6n5970B0NHPZfwDn9jSTX0d0ZvQfd5/PGgX38rP/D46PAaUk+SndDxB/QfdvKjxj6owB4Dt3P7zOAvZOcTPdYl137/TyO7g8KqU1zfQaKk5PT5pno7pws4Llj1J15AOsfD5Tdie6XZNEFw/Wtf1fgZXS/SH9G92iN/6Y7XXskcOeBuocz4mHAQ9vbne6OzivpHm57Ft2RxGX9useMWOe+dKHv2n6dL9NdgzbzLLXHj1hnO7qHG3+LLrz9jC5E/hvdI1y2WF+/B7ZTY0xb93Xv0H9W3+o/p5/S3aF7yIjtPoMu6FzUt+8autOILwbuOlDvQXSnQL/R17mBLtx9gDEfPDzQj1PpntV3fP/530h3SvLQDay7R7/+ZeN+biO2sQx4Zz8GN9JdL/kduodQP36o7jH9/nYD/rqvdyPdjQ1vA35jln08sP9M19IFtB/2n909Z6l/Z7o/es7tx+CndHfgvo2B5wcOtGfZiG3s3S971aT+P3By2pgpVXO9e16SNp8kH6I7knXfqrpg0u1pVbqvYfsA8HdV9YoNVJ+P/R1Dd83orlX1/c29P2kx8po6SQuuv+bqVnc1Jnk03anDbxvoNp/+ur8X0F1zN6dTr5Kml9fUSZqEreieyfYFutNw6+i+XmofulNsz55g25qV5A/pbozYG/hd4J1VdclEGyVp3hjqJE3CL+m+veBRdNeX3YnuESwfBV5fVaMew6FN9xi6b5m4hu4RJi+ebHMkzSevqZMkSWqA19RJkiQ1YNGfft12221r2bJlk26GJEnSBp111llXVdXSUcsWfahbtmwZq1evnnQzJEmSNijJD2Zb5ulXSZKkBhjqJEmSGmCokyRJaoChTpIkqQGGOkmSpAYY6iRJkhpgqJMkSWqAoU6SJKkBhjpJkqQGGOokSZIaYKiTJElqgKFOkiSpAYY6SZKkBhjqJEmSGmCokyRJaoChTpIkqQGGOkmSpAYY6iRJkhpgqJMkSWqAoU6SJKkBhjpJkqQGGOokSZIaYKiTJElqgKFOkiSpAYY6SZKkBhjqJEmSGmCokyRJaoChTpIkqQGGOkmSpAYY6iRJkhpgqJMkSWqAoU6SJKkBhjpJkqQGGOokSZIaYKiTJElqgKFOkiSpAYY6SZKkBhjqJEmSGmCokyRJasDUhbok+ya5IMmaJEeNWJ4k7+iXn5tkz6HlS5J8PcmnFq7VkiRJkzVVoS7JEuBdwH7A7sAhSXYfqrYfsLyfjgTeM7T8ecD5m7mpkiRJU2WqQh2wElhTVRdV1U3AicBBQ3UOAo6rzleArZPsCJBkZ+AA4F8WstGSJEmTNm2hbifg4oH5S/qyceu8DXgxcPP6dpLkyCSrk6xeu3btprVYkiRpCkxbqMuIshqnTpIDgSur6qwN7aSqjq6qFVW1YunSpRvTTkmSpKkybaHuEmCXgfmdgcvGrPNQ4HFJvk932vZRSY7ffE2VJEmaHtMW6s4ElifZNclWwMHASUN1TgKe0t8FuxdwXVVdXlUvraqdq2pZv97nq+pJC9p6SZKkCdli0g0YVFXrkjwHOBlYAry/qs5L8ox++XuBVcD+wBrgeuCISbVXkiRpWqRq+JK1xWXFihW1evXqSTdDkiRpg5KcVVUrRi2bttOvkiRJ2giGOkmSpAYY6iRJkhpgqJMkSWqAoU6SJKkBhjpJkqQGGOokSZIaYKiTJElqgKFOkiSpAYY6SZKkBhjqJEmSGmCokyRJaoChTpIkqQGGOkmSpAYY6iRJkhpgqJMkSWqAoU6SJKkBhjpJkqQGGOokSZIaYKiTJElqgKFOkiSpAYY6SZKkBhjqJEmSGmCokyRJaoChTpIkqQGGOkmSpAYY6iRJkhpgqJMkSWqAoU6SJKkBhjpJkqQGGOokSZIaYKiTJElqgKFOkiSpAYY6SZKkBhjqJEmSGmCokyRJaoChTpIkqQGGOkmSpAYY6iRJkhpgqJMkSWqAoU6SJKkBhjpJkqQGGOokSZIaYKiTJElqgKFOkiSpAYY6SZKkBhjqJEmSGmCokyRJaoChTpIkqQGGOkmSpAZMXahLsm+SC5KsSXLUiOVJ8o5++blJ9uzLd0nyhSTnJzkvyfMWvvWSJEmTMVWhLskS4F3AfsDuwCFJdh+qth+wvJ+OBN7Tl68D/rqq7gfsBTx7xLqSJElNmqpQB6wE1lTVRVV1E3AicNBQnYOA46rzFWDrJDtW1eVVdTZAVf0UOB/YaSEbL0mSNCnTFup2Ai4emL+EWwezDdZJsgx4APDVUTtJcmSS1UlWr127dhObLEmSNHnTFuoyoqzmUifJXYCPAc+vqp+M2klVHV1VK6pqxdKlSze6sZIkSdNi2kLdJcAuA/M7A5eNWyfJlnSB7kNV9fHN2E5JkqSpMm2h7kxgeZJdk2wFHAycNFTnJOAp/V2wewHXVdXlSQK8Dzi/qt6ysM2WJEmarC0m3YBBVbUuyXOAk4ElwPur6rwkz+iXvxdYBewPrAGuB47oV38o8GTgm0nO6cteVlWrFrIPkiRJk5Cq4UvWFpcVK1bU6tWrJ90MSZKkDUpyVlWtGLVs2k6/SpIkaSMY6iRJkhpgqJMkSWqAoU6SJKkBhjpJkqQGGOokSZIaYKiTJElqgKFOkiSpAYY6SZKkBhjqJEmSGmCokyRJaoChTpIkqQGGOkmSpAYY6iRJkhpgqJMkSWqAoU6SJKkBhjpJkqQGGOokSZIaYKiTJElqgKFOkiSpAYY6SZKkBhjqJEmSGmCokyRJaoChTpIkqQGGOkmSpAYY6iRJkhpgqJMkSWqAoU6SJKkBhjpJkqQGGOokSZIaYKiTJElqgKFOkiSpAYY6SZKkBhjqJEmSGmCokyRJaoChTpIkqQGGOkmSpAYY6iRJkhpgqJMkSWqAoU6SJKkB8xLqktxjPrYjSZKkjTOnUJfkaUleNDD/u0kuAa5MsjrJDvPeQkmSJG3QXI/U/RVww8D8W4BrgecDdwNeM0/tkiRJ0hxsMcf69wK+A5DkbsAjgMdX1aokVwOvm+f2SZIkaQxzPVK3BLi5f/+HQAGn9vMXA9vNT7MkSZI0F3MNdRcCB/TvDwbOqKrr+/l7AtfMV8MkSZI0vrmefn0z8MEkhwHbAH8+sOyRwLnz1TBJkiSNb06hrqpOSPJD4EHAmVV1+sDiK4CT5rNxkiRJGs9cj9RRVV8CvjSi/JXz0iJJkiTN2VyfU/eQJAcOzN8jyYeTfDPJm5Msmf8mSpIkaUPmeqPE64E/GJh/E7A/8F3gmcDLNrVBSfZNckGSNUmOGrE8Sd7RLz83yZ7jritJktSquYa6+wGrAZJsCfwZ8H+q6k+BlwOHbkpj+iN97wL2A3YHDkmy+1C1/YDl/XQk8J45rCtJktSkuV5TdxfgJ/37lcCdgU/182fTPZx4U6wE1lTVRQBJTgQOAr49UOcg4LiqKuArSbZOsiOwbIx1F9wp376CL164loctX8o+u28/yaZogTn2i5djv3g59ovXNIz9XI/UXQrs0b/fD/hWVV3Zz28DXD9yrfHtRPcQ4xmX9GXj1Bln3QV1yrev4Lkf/jrHffkHPPfDX+eUb18xyeZoATn2i5djv3g59ovXtIz9XEPdh4HXJvk34AXA8QPL9qR7OPGmyIiyGrPOOOt2G0iOTLI6yeq1a9fOsYnj++KFa7nhl78C4IZf/oovXrj59qXp4tgvXo794uXYL17TMvZzDXWvAt4A3J7upom3DizbA/joJrbnEmCXgfmdgcvGrDPOugBU1dFVtaKqVixdunQTmzy7hy1fyh237G4IvuOWS3jY8s23L00Xx37xcuwXL8d+8ZqWsU93adp0SLIF3Z20j6Y71XsmcGhVnTdQ5wDgOXR33T4IeEdVrRxn3VFWrFhRq1ev3hzdAabjHLsmw7FfvBz7xcuxX7wWauyTnFVVK0Yu25hQl+T+wCOAuwNXA6dX1bc2qZW/3vb+wNuAJcD7q+rvkzwDoKremyTAO4F96a7hO6KqVs+27ob2t7lDnSRJ0nyZt1DXHw07BjiEW17DVsAJwOFV9auNb+rCM9RJkqTbivWFurleU/dK4AnA3wC7AnfsX/8GeGL/KkmSpAU21+fUPQn426HTmj8A/r5/+O8RdMFPkiRJC2iuR+ruCXx5lmVn9MslSZK0wOYa6i4DHjrLsocwyyNEJEmStHnN9fTrh4CXJ7m5f385sANwMN13v75hfpsnSZKkccw11L0K2A14df9+Rujufn31vLRKkiRJczKnUFdV64BDk/w98HC659RdA5xGdz3d14Hfm+9GSpIkaf3meqQOgP5bGm7xTQ1J7gf8znw0SpIkSXMz1xslJEmSNIUMdZIkSQ0w1EmSJDVgg9fUJdltzG3tsIltkSRJ0kYa50aJNUCNUS9j1pMkSdI8GyfUHbHZWyFJkqRNssFQV1XHLkRDJEmStPG8UUKSJKkBhjpJkqQGGOokSZIaYKiTJElqgKFOkiSpAYY6SZKkBhjqJEmSGmCokyRJaoChTpIkqQGGOkmSpAYY6iRJkhpgqJMkSWqAoU6SJKkBhjpJkqQGGOokSZIaYKiTJElqgKFOkiSpAYY6SZKkBhjqJEmSGmCokyRJaoChTpIkqQGGOkmSpAYY6iRJkhpgqJMkSWqAoU6SJKkBhjpJkqQGGOokSZIaYKiTJElqgKFOkiSpAYY6SZKkBhjqJEmSGmCokyRJaoChTpIkqQGGOkmSpAYY6iRJkhpgqJMkSWrA1IS6JHdPckqSC/vXbWapt2+SC5KsSXLUQPmbknwnyblJ/j3J1gvXekmSpMmamlAHHAV8rqqWA5/r528hyRLgXcB+wO7AIUl27xefAty/qn4P+C7w0gVptSRJ0hSYplB3EHBs//5Y4PEj6qwE1lTVRVV1E3Bivx5V9ZmqWtfX+wqw82ZuryRJ0tSYplC3fVVdDtC/bjeizk7AxQPzl/Rlw/4C+PRsO0pyZJLVSVavXbt2E5osSZI0HbZYyJ0l+Syww4hFLx93EyPKamgfLwfWAR+abSNVdTRwNMCKFStqtnqSJEm3FQsa6qrqMbMtS3JFkh2r6vIkOwJXjqh2CbDLwPzOwGUD2zgMOBB4dFUZ1iRJ0qIxTadfTwIO698fBnxyRJ0zgeVJdk2yFXBwvx5J9gVeAjyuqq5fgPZKkiRNjWkKda8H9klyIbBPP0+SeyZZBdDfCPEc4GTgfOAjVXVev/47gbsCpyQ5J8l7F7oDkiRJk7Kgp1/Xp6quBh49ovwyYP+B+VXAqhH17rNZGyhJkjTFpulInSRJkjaSoU6SJKkBhjpJkqQGGOokSZIaYKiTJElqgKFOkiSpAYY6SZKkBhjqJEmSGmCokyRJaoChTpIkqQGGOkmSpAYY6iRJkhpgqJMkSWqAoU6SJKkBhjpJkqQGGOokSZIaYKiTJElqgKFOkiSpAYY6SZKkBhjqJEmSGmCokyRJaoChTpIkqQGGOkmSpAYY6iRJkhpgqJMkSWqAoU6SJKkBhjpJkqQGGOokSZIaYKiTJElqgKFOkiSpAYY6SZKkBhjqJEmSGmCokyRJaoChTpIkqQGGOkmSpAYY6iRJkhpgqJMkSWqAoU6SJKkBhjpJkqQGGOokSZIaYKiTJElqgKFOkiSpAYY6SZKkBhjqJEmSGmCokyRJaoChTpIkqQGGOkmSpAYY6iRJkhpgqJMkSWqAoU6SJKkBUxPqktw9ySlJLuxft5ml3r5JLkiyJslRI5a/MEkl2Xbzt1qSJGk6TE2oA44CPldVy4HP9fO3kGQJ8C5gP2B34JAkuw8s3wXYB/jhgrRYkiRpSkxTqDsIOLZ/fyzw+BF1VgJrquqiqroJOLFfb8ZbgRcDtTkbKkmSNG2mKdRtX1WXA/Sv242osxNw8cD8JX0ZSR4HXFpV39jQjpIcmWR1ktVr167d9JZLkiRN2BYLubMknwV2GLHo5eNuYkRZJblTv43HjrORqjoaOBpgxYoVHtWTJEm3eQsa6qrqMbMtS3JFkh2r6vIkOwJXjqh2CbDLwPzOwGXAbwK7At9IMlN+dpKVVfWjeeuAJEnSlJqm068nAYf17w8DPjmizpnA8iS7JtkKOBg4qaq+WVXbVdWyqlpGF/72NNBJkqTFYppC3euBfZJcSHcH6+sBktwzySqAqloHPAc4GTgf+EhVnTeh9kqSJE2NBT39uj5VdTXw6BHllwH7D8yvAlZtYFvL5rt9kiRJ02yajtRJkiRpIxnqJEmSGmCokyRJaoChTpIkqQGGOkmSpAYY6iRJkhpgqJMkSWqAoU6SJKkBhjpJkqQGGOokSZIaYKiTJElqgKFOkiSpAYY6SZKkBhjqJEmSGmCokyRJaoChTpIkqQGGOkmSpAYY6iRJkhpgqJMkSWqAoU6SJKkBhjpJkqQGGOokSZIaYKiTJElqgKFOkiSpAYY6SZKkBhjqJEmSGmCokyRJaoChTpIkqQGGOkmSpAYY6iRJkhpgqJMkSWqAoU6SJKkBhjpJkqQGGOokSZIaYKiTJElqgKFOkiSpAYY6SZKkBhjqJEmSGmCokyRJakCqatJtmKgka4EfbObdbAtctZn3Mc0Wc/8Xc99hcfffvi9ei7n/i7nvsDD9v3dVLR21YNGHuoWQZHVVrZh0OyZlMfd/MfcdFnf/7fvi7Dss7v4v5r7D5Pvv6VdJkqQGGOokSZIaYKhbGEdPugETtpj7v5j7Dou7//Z98VrM/V/MfYcJ999r6iRJkhrgkTpJkqQGGOrmUZJ9k1yQZE2So0YsT5J39MvPTbLnJNq5OYzR972TXJfknH76m0m0c3NI8v4kVyb51izLmx13GKv/LY/9Lkm+kOT8JOcled6IOk2O/5h9b3ns75Dka0m+0ff/1SPqtDr24/S92bEHSLIkydeTfGrEssmNe1U5zcMELAG+B+wGbAV8A9h9qM7+wKeBAHsBX510uxew73sDn5p0WzdT/x8O7Al8a5blTY77HPrf8tjvCOzZv78r8N1F9O9+nL63PPYB7tK/3xL4KrDXIhn7cfre7Nj3/XsBcMKoPk5y3D1SN39WAmuq6qKqugk4EThoqM5BwHHV+QqwdZIdF7qhm8E4fW9WVZ0OXLOeKq2OOzBW/5tVVZdX1dn9+58C5wM7DVVrcvzH7Huz+vH8WT+7ZT8NX6Te6tiP0/dmJdkZOAD4l1mqTGzcDXXzZyfg4jWms2UAAAT6SURBVIH5S7j1f3Dj1LktGrdfD+4P1386ye8sTNOmQqvjPhfNj32SZcAD6I5aDGp+/NfTd2h47PtTcOcAVwKnVNWiGfsx+g7tjv3bgBcDN8+yfGLjbqibPxlRNvyXyzh1bovG6dfZdF9tsgfwj8AnNnurpker4z6u5sc+yV2AjwHPr6qfDC8esUoz47+Bvjc99lX1q6r6fWBnYGWS+w9VaXbsx+h7k2Of5EDgyqo6a33VRpQtyLgb6ubPJcAuA/M7A5dtRJ3bog32q6p+MnO4vqpWAVsm2XbhmjhRrY77WFof+yRb0oWaD1XVx0dUaXb8N9T31sd+RlVdC5wK7Du0qNmxnzFb3xse+4cCj0vyfbpLjR6V5PihOhMbd0Pd/DkTWJ5k1yRbAQcDJw3VOQl4Sn9nzF7AdVV1+UI3dDPYYN+T7JAk/fuVdD97Vy94Syej1XEfS8tj3/frfcD5VfWWWao1Of7j9L3xsV+aZOv+/R2BxwDfGarW6thvsO+tjn1VvbSqdq6qZXS/6z5fVU8aqjaxcd9iIXayGFTVuiTPAU6muxv0/VV1XpJn9MvfC6yiuytmDXA9cMSk2jufxuz7nwHPTLIOuAE4uPrbhG7rknyY7k6vbZNcAryS7sLhpsd9xhj9b3bs6f5qfzLwzf76IoCXAfeC5sd/nL63PPY7AscmWUIXWD5SVZ9aDP/nM17fWx77W5mWcfcbJSRJkhrg6VdJkqQGGOokSZIaYKiTJElqgKFOkiSpAYY6SZKkBhjqJC1aSQ5PUrNM106wXcf0j4eRpLH5nDpJgj+newr8oHWTaIgkbSxDnSTBOVW1ZtKNkKRN4elXSVqPgVO0D0/yiSQ/S3J1knf1X5E0WHfHJMcluSrJL5Kcm2T4K4Tov1Lvg0l+1Ne7KMnbR9R7QJIvJrk+yYUzT62XpFE8UidJsCTJ8P+HN1fVzQPzxwMfAd4NrAT+BrgzcDhAkjsDpwHb0H1d1sXAk4APJrlTVR3d19sV+Brd1we9EriQ7su/Hzu0/98ATgDeBryG7quG3pPkgqr6wjz0WVJjDHWSdOsvYgf4T+DAgflVVfXC/v1nkhTwmiSvrarv0oWu5cAjq+rUvt6nk2wP/F2S91XVr4BXA3cE9qiqywa2f+zQ/u8KPGsmwCU5nS74HQIY6iTdiqdfJQn+GHjg0PT8oTofGZo/ke7/0JX9/MOBSwcC3YzjgaXA7v38Y4FPDQW6Ua4fPCJXVb+gO6p3rw11RtLi5JE6SYJvjXGjxBWzzO/Uv94duHzEej8aWA5wD259p+0oPx5R9gvgDmOsK2kR8kidJI1n+1nmL+1frwF2GLHeTNnV/etV/DoIStK8MdRJ0nieMDR/MHAz3U0P0N0ksXOShw7VOxS4Eji/n/8McGCSHTdXQyUtTp5+lST4/STbjihfPfB+/yRvogtlK+nuXD2uv0kC4BjgecDHk7yc7hTr/wb2AZ7e3yRBv94BwBlJXgusoTtyt29V3erxJ5I0LkOdJMFHZylfOvD+ScBfA88EbgL+GZi5G5aq+nmSRwBvBF5Pd/fqBcCTq+r4gXrfT/Ig4O+A1/X1LgU+OW+9kbQopaom3QZJmlpJDgc+ACz3WyckTTOvqZMkSWqAoU6SJKkBnn6VJElqgEfqJEmSGmCokyRJaoChTpIkqQGGOkmSpAYY6iRJkhpgqJMkSWrA/wdqugi4KyWh8wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set up the number of perceptron per each layer:\n",
    "p = 4 # Layer 1\n",
    "q = 4 # Layer 2\n",
    "\n",
    "# Set up the Learning rate\n",
    "eta = 1/623\n",
    "\n",
    "# 0: Random initialize the relevant data \n",
    "w1 = 2*np.random.rand(p, X_train.shape[1]) - 0.5 # Layer 1\n",
    "b1 = np.random.rand(p)\n",
    "\n",
    "w2 = 2*np.random.rand(q , p) - 0.5  # Layer 2\n",
    "b2 = np.random.rand(q)\n",
    "\n",
    "wOut = 2*np.random.rand(q) - 0.5  # Output Layer\n",
    "bOut = np.random.rand(1)\n",
    "\n",
    "mu = []\n",
    "vec_y = []\n",
    "\n",
    "# Start looping over dataset\n",
    "for i in range(0, X_train.shape[0]):\n",
    "    # 1: input the data \n",
    "    x = X_train[i]\n",
    "    \n",
    "    # 2: Start the algorithm\n",
    "    # 2.1: Feed forward\n",
    "    z1 = ReLU_act(np.dot(w1, x) + b1) # output layer 1 \n",
    "    z2 = ReLU_act(np.dot(w2, z1) + b2) # output layer 2\n",
    "    y = sigmoid_act(np.dot(wOut, z2) + bOut) # Output of the Output layer\n",
    "    \n",
    "    #2.2: Compute the output layer's error\n",
    "    delta_Out =  (y - y_train[i]) * sigmoid_act(y, der=True)\n",
    "    \n",
    "    #2.3: Backpropagate\n",
    "    delta_2 = delta_Out * wOut * ReLU_act(z2, der=True) # Second Layer Error\n",
    "    delta_1 = np.dot(delta_2, w2) * ReLU_act(z1, der=True) # First Layer Error\n",
    "    \n",
    "    # 3: Gradient descent \n",
    "    wOut = wOut - eta*delta_Out*z2  # Outer Layer\n",
    "    bOut = bOut - eta*delta_Out\n",
    "    \n",
    "    w2 = w2 - eta*np.kron(delta_2, z1).reshape(q,p) # Hidden Layer 2\n",
    "    b2 = b2 - eta*delta_2\n",
    "    \n",
    "    w1 = w1 - eta*np.kron(delta_1, x).reshape(p, x.shape[0]) # Hidden Layer 1\n",
    "    b1 = b1 - eta*delta_1\n",
    "    \n",
    "    # 4. Computation of the loss function\n",
    "    mu.append((1/2)*(y - y_train[i])**2)\n",
    "    vec_y.append(y[0])\n",
    "\n",
    "    \n",
    "# Plotting the Cost function for each training data     \n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(np.arange(0, X_train.shape[0]), mu, alpha=0.3, s=4, label='mu')\n",
    "plt.title('Loss for each training data point', fontsize=20)\n",
    "plt.xlabel('Training data', fontsize=16)\n",
    "plt.ylabel('Loss', fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "# Plotting the average cost function over 5 training data    \n",
    "pino = []\n",
    "for i in range(0, 5):\n",
    "    pippo = 0\n",
    "    for m in range(0, 5):\n",
    "        pippo += vec_y[10*i+m]/10\n",
    "    pino.append(pippo)\n",
    "    \n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(np.arange(0, 5), pino, alpha=1, s=10, label='error')\n",
    "plt.title('Average Loss by epoch', fontsize=20)\n",
    "plt.xlabel('Epoch', fontsize=16)\n",
    "plt.ylabel('Loss', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Training as a Function\n",
    "\n",
    "Since we have learnt how to train an ANN, we are in the position of define a function that does that, by eating the X_train, Y_train as well as the number of perceptron $p,q$ for the first and second hidden layer, and the learning rate $\\eta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ANN_train(X_train, y_train, p=4, q=4, eta=0.0015):\n",
    "    # 0: Random initialize the relevant data \n",
    "    w1 = 2*np.random.rand(p , X_train.shape[1]) - 0.5 # Layer 1\n",
    "    b1 = np.random.rand(p)\n",
    "\n",
    "    w2 = 2*np.random.rand(q , p) - 0.5  # Layer 2\n",
    "    b2 = np.random.rand(q)\n",
    "\n",
    "    wOut = 2*np.random.rand(q) - 0.5   # Output Layer\n",
    "    bOut = np.random.rand(1)\n",
    "\n",
    "    mu = []\n",
    "    vec_y = []\n",
    "\n",
    "    # Start looping over dataset\n",
    "    for i in range(0, X_train.shape[0]-1): \n",
    "        # 1: input the data \n",
    "        x = X_train[i]\n",
    "    \n",
    "        # 2: Start the algorithm\n",
    "        # 2.1: Feed forward\n",
    "        z1 = ReLU_act(np.dot(w1, x) + b1) # output layer 1 \n",
    "        z2 = ReLU_act(np.dot(w2, z1) + b2) # output layer 2\n",
    "        y = sigmoid_act(np.dot(wOut, z2) + bOut) # Output of the Output layer\n",
    "    \n",
    "        #2.2: Compute the output layer's error\n",
    "        delta_Out = 2 * (y-y_train[i]) * sigmoid_act(y, der=True)\n",
    "    \n",
    "        #2.3: Backpropagate\n",
    "        delta_2 = delta_Out * wOut * ReLU_act(z2, der=True) # Second Layer Error\n",
    "        delta_1 = np.dot(delta_2, w2) * ReLU_act(z1, der=True) # First Layer Error\n",
    "    \n",
    "        # 3: Gradient descent \n",
    "        wOut = wOut - eta*delta_Out*z2  # Outer Layer\n",
    "        bOut = bOut - eta*delta_Out\n",
    "    \n",
    "        w2 = w2 - eta*np.kron(delta_2, z1).reshape(q,p) # Hidden Layer 2\n",
    "        b2 = b2 -  eta*delta_2\n",
    "    \n",
    "        w1 = w1 - eta*np.kron(delta_1, x).reshape(p, x.shape[0])\n",
    "        b1 = b1 - eta*delta_1\n",
    "    \n",
    "        # 4. Computation of the loss function\n",
    "        mu.append((y-y_train[i])**2)\n",
    "        vec_y.append(y)\n",
    "    \n",
    "    batch_loss = []\n",
    "    for i in range(0, 5):\n",
    "        loss_avg = 0\n",
    "        for m in range(0, 5):\n",
    "            loss_avg += vec_y[10*i+m]/10\n",
    "        batch_loss.append(loss_avg)\n",
    "    \n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.scatter(np.arange(1, len(batch_loss)+1), batch_loss, alpha=1, s=10, label='error')\n",
    "    plt.title('Averege Loss by epoch', fontsize=20)\n",
    "    plt.xlabel('Epoch', fontsize=16)\n",
    "    plt.ylabel('Loss', fontsize=16)\n",
    "    plt.show()\n",
    "    \n",
    "    return w1, b1, w2, b2, wOut, bOut, mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "w1, b1, w2, b2, wOut, bOut, mu = ANN_train(X_train, y_train, p=8, q=4, eta=0.0015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Predictions\n",
    "\n",
    "We now have to compute predictions from our trained ANN. In order to do so we need to recall the trained parameters $\\{w\\}, \\{b\\}$ and use them to actually get the predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ANN_pred(X_test, w1, b1, w2, b2, wOut, bOut, mu):\n",
    "    pred = []\n",
    "    \n",
    "    for i in range(0, X_test.shape[0]): \n",
    "        # 1: input the data \n",
    "        x = X_test[i]\n",
    "        \n",
    "        # 2.1: Feed forward\n",
    "        z1 = ReLU_act(np.dot(w1, x) + b1) # output layer 1 \n",
    "        z2 = ReLU_act(np.dot(w2, z1) + b2) # output layer 2\n",
    "        y = sigmoid_act(np.dot(wOut, z2) + bOut)  # Output of the Output layer\n",
    "        \n",
    "        # Append the prediction;\n",
    "        # We now need a binary classifier; we this apply an Heaviside Theta and we set to 0.5 the threshold\n",
    "        # if y < 0.5 the output is zero, otherwise is 1\n",
    "        pred.append(np.heaviside(y - 0.5, 1)[0])\n",
    "    \n",
    "    return np.array(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = ANN_pred(X_test, w1, b1, w2, b2, wOut, bOut, mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_labels = {\n",
    "    0: \"Poor\",\n",
    "    1: \"Good\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot the confusion matrix\n",
    "# cm = confusion_matrix(y_test, predictions)\n",
    "\n",
    "# df_cm = pd.DataFrame(cm, index = [dict_labels[i] for i in range(0,2)], columns = [dict_labels[i] for i in range(0,2)])\n",
    "# plt.figure(figsize = (7,7))\n",
    "# sns.heatmap(df_cm, annot=True, cmap=plt.cm.Blues, fmt='g')\n",
    "# plt.xlabel(\"Predicted Class\", fontsize=18)\n",
    "# plt.ylabel(\"True Class\", fontsize=18)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the confusion matrix\n",
    "# From https://stackoverflow.com/a/44193420\n",
    "\n",
    "def pretty_print_conf_matrix(y_true, y_pred, \n",
    "                             classes,\n",
    "                             normalize=False,\n",
    "                             title='Confusion matrix',\n",
    "                             cmap=plt.cm.Blues,\n",
    "                             no=0):\n",
    "    \"\"\"\n",
    "    Mostly stolen from: http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py\n",
    "\n",
    "    Normalization changed, classification_report stats added below plot\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Configure Confusion Matrix Plot Aesthetics (no text yet) \n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, fontsize=14)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    plt.ylabel('True label', fontsize=12)\n",
    "    plt.xlabel('Predicted label', fontsize=12)\n",
    "\n",
    "    # Calculate normalized values (so all cells sum to 1) if desired\n",
    "    if normalize:\n",
    "        cm = np.round(cm.astype('float') / cm.sum(),2) #(axis=1)[:, np.newaxis]\n",
    "\n",
    "    # Place Numbers as Text on Confusion Matrix Plot\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\",\n",
    "                 fontsize=12)\n",
    "\n",
    "\n",
    "    # Add Precision, Recall, F-1 Score as Captions Below Plot\n",
    "    rpt = classification_report(y_true, y_pred)\n",
    "    rpt = rpt.replace('avg / total', '      avg')\n",
    "    rpt = rpt.replace('support', 'N Obs')\n",
    "\n",
    "    plt.annotate(rpt, \n",
    "                 xy = (0,0), \n",
    "                 xytext = (-50, -140), \n",
    "                 xycoords='axes fraction', textcoords='offset points',\n",
    "                 fontsize=12, ha='left')    \n",
    "\n",
    "    # Plot\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save picture\n",
    "    os.chdir(pictureDir)\n",
    "    plt.savefig(no + \"_Confusion_Matrix.png\")  \n",
    "    os.chdir(currentDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/kellie/Downloads/UROP Computational Tool for Biomedical Data Analysis/MLP/NN/Outputs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-333-e6e6b6b41da1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m pretty_print_conf_matrix(y_test, predictions, \n\u001b[0m\u001b[1;32m      2\u001b[0m                          \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                          \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                          title='Confusion Matrix', no=1)\n",
      "\u001b[0;32m<ipython-input-332-71d42e55f04c>\u001b[0m in \u001b[0;36mpretty_print_conf_matrix\u001b[0;34m(y_true, y_pred, classes, normalize, title, cmap, no)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;31m# Save picture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpictureDir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_Confusion_Matrix.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrentDir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/kellie/Downloads/UROP Computational Tool for Biomedical Data Analysis/MLP/NN/Outputs'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pretty_print_conf_matrix(y_test, predictions, \n",
    "                         classes= ['0', '1'],\n",
    "                         normalize=True, \n",
    "                         title='Confusion Matrix', \n",
    "                         no=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course the outcome is **not** amazing, but we have to keep in mind that this is a very simple (almost naive) implementation of an artificial neural network whose main goal was to explain (mostly to myself) how an ANN works in practice and how it is possible to implement it into *Python* from scratch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The ANN as a Class\n",
    "\n",
    "We want now to implement the code as Class for Python, so that we could easiliy generalize it; the goal is to have a class from which we can instantiate an object \"Neural Network\", and add to it as many hidden layers with as many neurons we want, with the desired activation functions and so on. To generalize further, we define also the *Activation_function* class and the *layers* class, so we may easily add more activation funtions or more different layers (such as Convolutional or Pooling layers for Convolutional Neural Networks).\n",
    "\n",
    "Within the ANN class, we define the following methods:\n",
    "* **add**: it eats a tuple ( int(number_of_neurons), string(activation_function) ), i.e. the output of the ANN.layer method. It is a void method. It updates the HiddenLayer string defined by the __init__ method. \n",
    "* **FeedForward**: it implements the Feed Forward layer by layer.\n",
    "* **BackPropagation**: it implements the whole gradient descent mechanism; first, it computes the errors by implementing the backpropagation; then, it updates the ANN parameters by gradient descent. \n",
    "* **Fit**: this method eats the training features and labels and fits the ANN by calling iteratively *FeedForward* and *BackPropagation* methods. This allow us to easily modify (or generalize) either *FeedForward* or *BackPropagation* methods without altering the *Fit* method.\n",
    "* **predict**: it eats the featurs and spits the label predictions. \n",
    "* **set_learning_rate**: by default the learning rate is initialized to be 1, but we can call this method to set it to a different value. \n",
    "* **get_accuracy, get_avg_accuracy**: these methods' aim is to return the cost function either at each step of the training process or averaging over 10 passengers, respectively.\n",
    "\n",
    "In the **layers** class we have\n",
    "* **layer**: it eats two imputs, the number of neurons and the activation function (as a string); it returns a tuple of the two. The idea is to leave room for a generalization of the *layers.layer* method later on by adding multiple layer type (i.e. Pooling or Convolutional layers for the CNN).\n",
    "\n",
    "In the **Activation_function** class we have\n",
    "* **ReLU_act, sigmoid_act**: these are the activation functions. They can be easily generalized (LeakyReLU, ParametricReLU, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An initialization Improvement\n",
    "\n",
    "I recently found an interesting article on the web [1] (see [2] for an easy&fast review) where the authors were able to define a well-performing initialization method for the $\\{w\\}$ and $\\{b\\}$ of the ANN. It relies on the knowledge of the dimension of the previous layer; so they are gaussian-distributed randomly initialized values divided by the squareroot of the dimension of the previous layer, i.e.\n",
    "> w = np.random.randn(layer_size[l],layer_size[l-1])*np.sqrt(2/layer_size[l-1])\n",
    ">\n",
    "> b = np.random.randn(layer_size[l])*np.sqrt(2/layer_size[l-1])\n",
    "\n",
    "Using that, as well as *Parametric Rectified Linear Units* (P-ReLU), they were able to obtain a result that\n",
    "> [...] *is the first to surpass human-level performance (5.1%, Russakovsky et al.) on this visual recognition challenge.*\n",
    "\n",
    "The visual recognition challenge they are referring to is the **ImageNet Large Scale Visual Recognition Competition (ILSVRC) 2014**.\n",
    "\n",
    "-----\n",
    "[1] https://arxiv.org/abs/1502.01852\n",
    "\n",
    "[2] https://towardsdatascience.com/random-initialization-for-neural-networks-a-thing-of-the-past-bfcdd806bf9e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Artificial Neural Network Class\n",
    "'''\n",
    "class ANN:    \n",
    "    '''\n",
    "    Initialize the ANN;\n",
    "    HiddenLayer vector : will contain the Layers' info\n",
    "    w, b, phi = (empty) arrays that will contain all the w, b and activation functions for all the Layers\n",
    "    mu = cost function\n",
    "    eta = a standard learning rate initialization. It can be modified by the 'set_learning_rate' method\n",
    "    '''\n",
    "    def __init__(self) :\n",
    "        self.HiddenLayer = []\n",
    "        self.w = []\n",
    "        self.b = []\n",
    "        self.phi = []\n",
    "        self.mu = []\n",
    "        self.eta = 1 #set up the proper Learning Rate!!\n",
    "    \n",
    "    '''\n",
    "    add method: to add layers to the network\n",
    "    '''\n",
    "    def add(self, lay = (4, 'ReLU') ):\n",
    "        self.HiddenLayer.append(lay)\n",
    "    \n",
    "    '''\n",
    "    FeedForward method: as explained before. \n",
    "    '''\n",
    "    @staticmethod\n",
    "    def FeedForward(w, b, phi, x):\n",
    "        return phi(np.dot(w, x) + b)\n",
    "        \n",
    "    '''\n",
    "    BackPropagation algorithm implementing the Gradient Descent \n",
    "    '''\n",
    "    def BackPropagation(self, x, z, Y, w, b, phi):\n",
    "        self.delta = []\n",
    "        \n",
    "        # We initialize ausiliar w and b that are used only inside the backpropagation algorithm once called        \n",
    "        self.W = []\n",
    "        self.B = []\n",
    "        \n",
    "        # We start computing the LAST error, the one for the OutPut Layer \n",
    "        self.delta.append(  (z[len(z)-1] - Y) * phi[len(z)-1](z[len(z)-1], der=True) )\n",
    "        \n",
    "        '''Now we BACKpropagate'''\n",
    "        # We thus compute from next-to-last to first\n",
    "        for i in range(0, len(z)-1):\n",
    "            self.delta.append( np.dot( self.delta[i], w[len(z)- 1 - i] ) * phi[len(z)- 2 - i](z[len(z)- 2 - i], der=True) )\n",
    "        \n",
    "        # We have the error array ordered from last to first; we flip it to order it from first to last\n",
    "        self.delta = np.flip(self.delta, 0)  \n",
    "        \n",
    "        # Now we define the delta as the error divided by the number of training samples\n",
    "        self.delta = self.delta/self.X.shape[0] \n",
    "        \n",
    "        '''GRADIENT DESCENT'''\n",
    "        # We start from the first layer that is special, since it is connected to the Input Layer\n",
    "        self.W.append( w[0] - self.eta * np.kron(self.delta[0], x).reshape( len(z[0]), x.shape[0] ) )\n",
    "        self.B.append( b[0] - self.eta * self.delta[0] )\n",
    "        \n",
    "        # We now descend for all the other Hidden Layers + OutPut Layer\n",
    "        for i in range(1, len(z)):\n",
    "            self.W.append( w[i] - self.eta * np.kron(self.delta[i], z[i-1]).reshape(len(z[i]), len(z[i-1])) )\n",
    "            self.B.append( b[i] - self.eta * self.delta[i] )\n",
    "        \n",
    "        # We return the descended parameters w, b\n",
    "        return np.array(self.W), np.array(self.B)\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Fit method: it calls FeedForward and Backpropagation methods\n",
    "    '''\n",
    "    def Fit(self, X_train, y_train):            \n",
    "        print('Start fitting...')\n",
    "        '''\n",
    "        Input layer\n",
    "        '''\n",
    "        self.X = X_train\n",
    "        self.Y = y_train\n",
    "        \n",
    "        '''\n",
    "        We now initialize the Network by retrieving the Hidden Layers and concatenating them \n",
    "        '''\n",
    "        print('Model recap: \\n')\n",
    "        print('You are fitting an ANN with the following amount of layers: ', len(self.HiddenLayer))\n",
    "        \n",
    "        for i in range(0, len(self.HiddenLayer)) :\n",
    "            print('Layer ', i+1)\n",
    "            print('Number of neurons: ', self.HiddenLayer[i][0])\n",
    "            if i==0:\n",
    "                # We now try to use the He et al. Initialization from ArXiv:1502.01852\n",
    "                self.w.append( np.random.randn(self.HiddenLayer[i][0] , self.X.shape[1])/np.sqrt(2/self.X.shape[1]) )\n",
    "                self.b.append( np.random.randn(self.HiddenLayer[i][0])/np.sqrt(2/self.X.shape[1]))\n",
    "                # Old initialization\n",
    "                #self.w.append(2 * np.random.rand(self.HiddenLayer[i][0] , self.X.shape[1]) - 0.5)\n",
    "                #self.b.append(np.random.rand(self.HiddenLayer[i][0]))\n",
    "                \n",
    "                # Initialize the Activation function\n",
    "                for act in Activation_function.list_act():\n",
    "                    if self.HiddenLayer[i][1] == act :\n",
    "                        self.phi.append(Activation_function.get_act(act))\n",
    "                        print('\\tActivation: ', act)\n",
    "\n",
    "            else :\n",
    "                # We now try to use the He et al. Initialization from ArXiv:1502.01852\n",
    "                self.w.append( np.random.randn(self.HiddenLayer[i][0] , self.HiddenLayer[i-1][0] )/np.sqrt(2/self.HiddenLayer[i-1][0]))\n",
    "                self.b.append( np.random.randn(self.HiddenLayer[i][0])/np.sqrt(2/self.HiddenLayer[i-1][0]))\n",
    "                # Old initialization\n",
    "                #self.w.append(2*np.random.rand(self.HiddenLayer[i][0] , self.HiddenLayer[i-1][0] ) - 0.5)\n",
    "                #self.b.append(np.random.rand(self.HiddenLayer[i][0]))\n",
    "                \n",
    "                # Initialize the Activation function\n",
    "                for act in Activation_function.list_act():\n",
    "                    if self.HiddenLayer[i][1] == act :\n",
    "                        self.phi.append(Activation_function.get_act(act))\n",
    "                        print('\\tActivation: ', act)\n",
    "            \n",
    "        '''\n",
    "        Now we start the Loop over the training dataset\n",
    "        '''  \n",
    "        for j in range(0, self.X.shape[0]): # loop over the training set\n",
    "            '''\n",
    "            Now we start the feed forward\n",
    "            '''  \n",
    "            self.z = []\n",
    "            \n",
    "            self.z.append( self.FeedForward(self.w[0], self.b[0], self.phi[0], self.X[j]) ) # First layer\n",
    "            \n",
    "            for i in range(1, len(self.HiddenLayer)): #Looping over layers\n",
    "                self.z.append( self.FeedForward(self.w[i] , self.b[i], self.phi[i], self.z[i-1] ) )\n",
    "        \n",
    "            \n",
    "            '''\n",
    "            Here we backpropagate\n",
    "            '''      \n",
    "            self.w, self.b  = self.BackPropagation(self.X[I], self.z, self.Y[I], self.w, self.b, self.phi)\n",
    "            \n",
    "            '''\n",
    "            Compute cost function\n",
    "            ''' \n",
    "            self.mu.append(\n",
    "                (1/2) * np.dot(self.z[len(self.z)-1] - self.Y[I], self.z[len(self.z)-1] - self.Y[j]) \n",
    "            )\n",
    "            \n",
    "        print('Fit done. \\n')\n",
    "\n",
    "    '''\n",
    "    predict method\n",
    "    '''\n",
    "    def predict(self, X_test):\n",
    "        print('Starting predictions...')\n",
    "        \n",
    "        self.pred = []\n",
    "        self.XX = X_test\n",
    "        \n",
    "        for j in range(0, self.XX.shape[0]): # loop over the training set\n",
    "            '''\n",
    "            Now we start the feed forward\n",
    "            '''  \n",
    "            self.z = []\n",
    "            \n",
    "            self.z.append(self.FeedForward(self.w[0] , self.b[0], self.phi[0], self.XX[j])) #First layer\n",
    "    \n",
    "            for i in range(1, len(self.HiddenLayer)) : # loop over the layers\n",
    "                self.z.append( self.FeedForward(self.w[i] , self.b[i], self.phi[i], self.z[i-1]))\n",
    "       \n",
    "            # Append the prediction;\n",
    "            # We now need a binary classifier; we this apply an Heaviside Theta and we set to 0.5 the threshold\n",
    "            # if y < 0.5 the output is zero, otherwise is zero\n",
    "            self.pred.append( np.heaviside(self.z[-1] - 0.5, 1)[0] ) # NB: self.z[-1] is the last element of the self.z list\n",
    "        \n",
    "        print('Predictions done. \\n')\n",
    "\n",
    "        return np.array(self.pred)\n",
    "   \n",
    "    '''\n",
    "    We need a method to retrieve the accuracy for each training data to follow the learning of the ANN\n",
    "    '''\n",
    "    def get_accuracy(self):\n",
    "        return np.array(self.mu)\n",
    "    \n",
    "    # This is the averaged version\n",
    "    def get_avg_accuracy(self):\n",
    "        import math\n",
    "        self.batch_loss = []\n",
    "        for i in range(0, 10):\n",
    "            self.loss_avg = 0\n",
    "            # To set the batch in 10 element/batch we use math.ceil method\n",
    "            # int(math.ceil((self.X.shape[0]-10) / 10.0))    - 1\n",
    "            for m in range(0, (int(math.ceil((self.X.shape[0]-10) / 10.0))   )-1):\n",
    "                #self.loss_avg += self.mu[60*i+m]/60\n",
    "                self.loss_avg += self.mu[(int(math.ceil((self.X.shape[0]-10) / 10.0)) )*i + m]/(int(math.ceil((self.X.shape[0]-10) / 10.0)) )\n",
    "            self.batch_loss.append(self.loss_avg)\n",
    "        return np.array(self.batch_loss)\n",
    "    \n",
    "    '''\n",
    "    Method to set the learning rate\n",
    "    '''\n",
    "    def set_learning_rate(self, et=1):\n",
    "        self.eta = et\n",
    "        \n",
    "'''\n",
    "layers class\n",
    "'''\n",
    "class layers :\n",
    "    '''\n",
    "    Layer method: used to call standar layers to add. \n",
    "    Easily generalizable to more general layers (Pooling and Convolutional layers)\n",
    "    '''        \n",
    "    def layer(p=4, activation='ReLU'):\n",
    "        return (p, activation)\n",
    "\n",
    "'''\n",
    "Activation functions class\n",
    "'''\n",
    "class Activation_function(ANN):\n",
    "    def __init__(self) :\n",
    "        super().__init__()\n",
    "        \n",
    "    '''\n",
    "    Define the sigmoid activator; we ask if we want the sigmoid or its derivative\n",
    "    '''\n",
    "    def sigmoid_act(x, der=False):\n",
    "        if der : #derivative of the sigmoid\n",
    "            f = 1/(1+ np.exp(-x))*(1-1/(1+ np.exp(-x)))\n",
    "        else : # sigmoid\n",
    "            f = 1/(1+ np.exp(-x))\n",
    "        return f\n",
    "\n",
    "    '''\n",
    "    Define the Rectifier Linear Unit (ReLU)\n",
    "    '''\n",
    "    def ReLU_act(x, der=False):\n",
    "        if der: # the derivative of the ReLU is the Heaviside Theta\n",
    "            f = np.heaviside(x, 1)\n",
    "        else :\n",
    "            f = np.maximum(x, 0)\n",
    "        return f\n",
    "    \n",
    "    def list_act():\n",
    "        return ['sigmoid', 'ReLU']\n",
    "    \n",
    "    def get_act(string='ReLU'):\n",
    "        if string == 'ReLU':\n",
    "            return ReLU_act\n",
    "        elif string == 'sigmoid':\n",
    "            return sigmoid_act\n",
    "        else:\n",
    "            return sigmoid_act"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiating the class and Fitting the model\n",
    "\n",
    "Now we instantiate our model, that will be exactly the same as before, i.e. a two-hidden layer with 8 and 4 neurons respectively and with ReLU activation plus an OutPut layer with a single neuron with sigmoid activation. \n",
    "\n",
    "We will set the learning rate and then fit the model; after that we recover the accuracy history (even in the averaged over 10 batches) and finally we compute the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ANN()\n",
    "\n",
    "model.add(layers.layer(8, 'ReLU'))\n",
    "model.add(layers.layer(4, 'ReLU'))\n",
    "model.add(layers.layer(1, 'sigmoid'))\n",
    "\n",
    "model.set_learning_rate(0.8)\n",
    "\n",
    "model.Fit(X_train, y_train)\n",
    "acc_val = model.get_accuracy()\n",
    "acc_avg_val = model.get_avg_accuracy()\n",
    "\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the accuracy scores we have retrieved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(np.arange(1, X_train.shape[0]+1), acc_val, alpha=0.3, s=4, label='mu')\n",
    "plt.title('Loss for each training data point', fontsize=20)\n",
    "plt.xlabel('Training data', fontsize=16)\n",
    "plt.ylabel('Loss', fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(np.arange(1, len(acc_avg_val)+1), acc_avg_val, label='mu')\n",
    "plt.title('Averege Loss by epoch', fontsize=20)\n",
    "plt.xlabel('Training data', fontsize=16)\n",
    "plt.ylabel('Loss', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally see how the system behaves as a classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot the confusion matrix\n",
    "# cm = confusion_matrix(y_test, predictions)\n",
    "\n",
    "# df_cm = pd.DataFrame(cm, index = [dict_labels[i] for i in range(0,2)], columns = [dict_labels[i] for i in range(0,2)])\n",
    "# plt.figure(figsize = (7,7))\n",
    "# sns.heatmap(df_cm, annot=True, cmap=plt.cm.Blues, fmt='g')\n",
    "# plt.xlabel(\"Predicted Class\", fontsize=18)\n",
    "# plt.ylabel(\"True Class\", fontsize=18)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_print_conf_matrix(y_test, predictions, \n",
    "                         classes= ['0', '1'],\n",
    "                         normalize=True, \n",
    "                         title='Confusion Matrix',\n",
    "                         no=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "Notice that, having changed the random initialization following the He et al. procedure, we have improved the classification slightly (recall that this still is a very naive model). Now we can easily add more layers and, in the spirit of Convolutional Neural Network, we can easily modify the class to add for more generic layers like *Convolutional* or *Pooling* layers.\n",
    "\n",
    "We can now try a more *deep* neural network for the case at hand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ANN()\n",
    "\n",
    "model.add(layers.layer(24, 'ReLU'))\n",
    "model.add(layers.layer(12, 'sigmoid'))\n",
    "model.add(layers.layer(6, 'ReLU'))\n",
    "model.add(layers.layer(1, 'sigmoid'))\n",
    "\n",
    "model.set_learning_rate(0.8)\n",
    "\n",
    "model.Fit(X_train, y_train)\n",
    "acc_val = model.get_accuracy()\n",
    "acc_avg_val = model.get_avg_accuracy()\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(np.arange(1, len(acc_avg_val)+1), acc_avg_val, label='mu')\n",
    "plt.title('Averege Loss by epoch', fontsize=20)\n",
    "plt.xlabel('Training data', fontsize=16)\n",
    "plt.ylabel('Loss', fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "pretty_print_conf_matrix(y_test, predictions, \n",
    "                         classes= ['0', '1'],\n",
    "                         normalize=True, \n",
    "                         title='Confusion Matrix',\n",
    "                         no=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
